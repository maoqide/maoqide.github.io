[
{
	"uri": "/cloud/deploy-mysql-on-kubernetes/",
	"title": "Deploy Mysql on Kubernetes",
	"tags": [],
	"description": "",
	"content": "本文通过 mysql-operator 在kubernetes集群部署高可用的mysql statefulset。\n环境准备 本文使用的开源 operator 项目 mysql-operator 配死只支持 mysql 8.0.11 以上的版本，改了下代码，支持 5.7.0 以上版本，项目地址，本文部署的是 mysql-5.7.26，使用的 dockerhub 上的镜像 mysql/mysql-server:5.7.26。\n代码编译 git clone 下载该项目，进入到代码目录，执行sh hack/build.sh，编译代码得到二进制文件 mysql-agent 和 mysql-operator，将二进制文件放入 bin/linux_amd64，执行docker build -f docker/mysql-agent/Dockerfile -t $IMAGE_NAME_AGENT .，docker build -f docker/mysql-operator/Dockerfile -t $IMAGE_NAME_OPERATOR .构建镜像，mysql-operator 生成的镜像为 operator 的镜像，mysql-agent 生成的是镜像，在创建mysql服务时，作为sidecar和mysql-server容器起在同一个pod中。\n部署 operator 先根据 文档 部署 mysql-operator 的 Deployment，文档中是使用 helm 安装，不希望安装 helm 和 tiller 的话，可以只安装一个 helm 客户端，进入到代码目录，再执行helm template --name mysql-operator mysql-operator生成部署所需要的yaml文件，然后直接执行kubectl apply -f mysql-operator.yaml创建 operator。这个yaml创建了operator所需的CRD类型，operator 的 Deployment 和 operator 所需的 RBAC 权限等。\n# change directory into mysql-operator cd mysql-operator # generate mysql-operator.yaml helm template --name mysql-operator mysql-operator \u0026gt; mysql-operator.yaml # deploy on kubernetes kubectl apply -f mysql-operator.yaml # deployed. [root@localhost]$ kubectl get deploy -n mysql-operator NAME READY UP-TO-DATE AVAILABLE AGE mysql-operator 1/1 1 1 2d5h  创建 mysql 集群 本文创建的集群为3节点的 mysql，一个节点为 master，二个为 slave，master节点可读写，slave节点为只读，使用 kubernetes Local PV 作持久化存储。\n首先，为每个节点创建一个PV，Local PV 需要定义nodeAffinity，约束创建的节点。\napiVersion: v1 kind: PersistentVolume metadata: name: mypv0 spec: capacity: storage: 1Gi volumeMode: Filesystem accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Delete storageClassName: mysql-storage local: path: /data/mysql-data nodeAffinity: required: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - 192.168.0.1 --- apiVersion: v1 kind: PersistentVolume metadata: name: mypv1 spec: capacity: storage: 1Gi volumeMode: Filesystem accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Delete storageClassName: mysql-storage local: path: /data/mysql-data nodeAffinity: required: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - 192.168.0.2 --- apiVersion: v1 kind: PersistentVolume metadata: name: mypv2 spec: capacity: storage: 1Gi volumeMode: Filesystem accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Delete storageClassName: mysql-storage local: path: /data/mysql-data nodeAffinity: required: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - 192.168.0.3  # create pv kubectl create -f pv.yaml # get presistence volume [root@localhost]$ kubectl get pv mypv-0 1Gi RWO Delete Available mysql-storage 4s mypv-1 1Gi RWO Delete Available mysql-storage 4s mypv-2 1Gi RWO Delete Available mysql-storage 4s  接着，需要在创建 mysql 的 namespace 下，为要创建的 mysql 创建对应的 RBAC 权限。\napiVersion: v1 kind: ServiceAccount metadata: name: mysql-agent namespace: mysql2 --- kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: mysql-agent namespace: mysql2 roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: mysql-agent subjects: - kind: ServiceAccount name: mysql-agent namespace: mysql2  如果需要自定义 mysql 的密码，需要为其创建一个 secret，密码需要使用base64加密。linux 下执行 echo -n 'password' | base64 为密码加密。\napiVersion: v1 data: password: cm9vdA== kind: Secret metadata: labels: v1alpha1.mysql.oracle.com/cluster: mysql name: mysql-pv-root-password namespace: mysql2  kubectl apply -f rbac.yaml kubectl apply -f secret.yaml  在创建 operator 的时候，已经创建了如下的crd类型，部署mysql集群所需创建的就是 mysqlclusters 类型的资源。\n[root@localhost]$ kubectl get crd | grep mysql mysqlbackups.mysql.oracle.com 2019-05-14T02:51:11Z mysqlbackupschedules.mysql.oracle.com 2019-05-14T02:51:11Z mysqlclusters.mysql.oracle.com 2019-05-14T02:51:11Z mysqlrestores.mysql.oracle.com 2019-05-14T02:51:11Z  接下来开始创建 operator 自定义资源类型(CRD)的实例 mysqlclusters。\napiVersion: mysql.oracle.com/v1alpha1 kind: Cluster metadata: name: mysql namespace: mysql2 spec: # 和mysql-server镜像版本的tag一直 version: 5.7.26 repository: 20.26.28.56/dcos/mysql-server # 节点数量 members: 3 # 指定 mysql 密码，和之前创建的secret名称一致 rootPasswordSecret: name: mysql-pv-root-password resources: agent: limits: cpu: 500m memory: 200Mi requests: cpu: 300m memory: 100Mi server: limits: cpu: 1000m memory: 1000Mi requests: cpu: 500m memory: 500Mi volumeClaimTemplate: metadata: name: mysql-pv spec: accessModes: [ \u0026quot;ReadWriteOnce\u0026quot; ] storageClassName: \u0026quot;mysql-storage\u0026quot; resources: requests: storage: 1Gi  kubectl apply -f mysql.yaml  执行后，会看到 kubernetes 在该 namespace 下开始拉起 mysql 的 statefulset，并会创建一个 headless service。\n[root@localhost]$ kubectl get all -n mysql2 NAME READY STATUS RESTARTS AGE pod/mysql-0 2/2 Running 0 8h pod/mysql-1 2/2 Running 0 8h pod/mysql-2 2/2 Running 0 8h NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/mysql ClusterIP None \u0026lt;none\u0026gt; 3306/TCP 21h NAME READY AGE statefulset.apps/mysql 1/1 21h  此时执行hack/cluster-status.sh脚本，会得到如下集群信息：\n{ \u0026quot;clusterName\u0026quot;: \u0026quot;Cluster\u0026quot;, \u0026quot;defaultReplicaSet\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;primary\u0026quot;: \u0026quot;mysql-0.mysql:3306\u0026quot;, \u0026quot;ssl\u0026quot;: \u0026quot;DISABLED\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;OK_NO_TOLERANCE\u0026quot;, \u0026quot;statusText\u0026quot;: \u0026quot;Cluster is NOT tolerant to any failures. 2 members are not active\u0026quot;, \u0026quot;topology\u0026quot;: { \u0026quot;mysql-0.mysql:3306\u0026quot;: { \u0026quot;address\u0026quot;: \u0026quot;mysql-0.mysql:3306\u0026quot;, \u0026quot;mode\u0026quot;: \u0026quot;R/W\u0026quot;, \u0026quot;readReplicas\u0026quot;: {}, \u0026quot;role\u0026quot;: \u0026quot;HA\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;ONLINE\u0026quot; }, \u0026quot;mysql-1.mysql:3306\u0026quot;: { \u0026quot;address\u0026quot;: \u0026quot;mysql-1.mysql:3306\u0026quot;, \u0026quot;mode\u0026quot;: \u0026quot;n/a\u0026quot;, \u0026quot;readReplicas\u0026quot;: {}, \u0026quot;role\u0026quot;: \u0026quot;HA\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;ONLINE\u0026quot; }, \u0026quot;mysql-2.mysql:3306\u0026quot;: { \u0026quot;address\u0026quot;: \u0026quot;mysql-2.mysql:3306\u0026quot;, \u0026quot;mode\u0026quot;: \u0026quot;n/a\u0026quot;, \u0026quot;readReplicas\u0026quot;: {}, \u0026quot;role\u0026quot;: \u0026quot;HA\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;ONLINE\u0026quot; } }, \u0026quot;topologyMode\u0026quot;: \u0026quot;Single-Primary\u0026quot; }, \u0026quot;groupInformationSourceMember\u0026quot;: \u0026quot;mysql-0.mysql:3306\u0026quot; }  通过DNS地址 mysql-0.mysql.mysql2.svc.cluster.local:3306 可以连接到数据库进行读写操作。此时一个多节点的mysql集群已经部署完成，但是，集群外部的服务还无法访问数据库。\n通过 haproxy-ingress 允许外部访问 首先，headless service只能通过集群内 DNS 访问服务，要外部访问，还需要另外创建一个 Service。为了让外部可以访问到 mysql-0 的服务，我们为 mysql-0 创建一个ClusterIP 类型的服务。\nkind: Service apiVersion: v1 metadata: name: mysql-0 namespace: mysql2 spec: selector: # 通过 selector 将 pod 约束到 mysql-0 statefulset.kubernetes.io/pod-name: mysql-0 ports: - protocol: TCP port: 3306 targetPort: 3306  接着，需要创建一个ingress-controller，本文选用的是 haproxy-ingress。\n由于 mysql 服务通过 TCP 协议通信，kubernetes ingress 默认只支持 http 和 https，haproxy-ingress 提供了通过 configmap 的方法，配置 TCP 服务的端口，需要先创建一个 configmap，configmap的data中，key为HAProxy监听的端口，value 为需要转发的 service 的服务和端口。\napiVersion: v1 kind: ConfigMap metadata: name: mysql-tcp namespace: mysql2 data: \u0026quot;3306\u0026quot;: \u0026quot;mysql2/mysql-0:3306\u0026quot;  kubectl apply -f mysql-0.yaml kubectl apply -f tcp-svc.yaml  接下来创建 ingress-controller，\napiVersion: extensions/v1beta1 kind: Deployment metadata: labels: run: haproxy-ingress name: haproxy-ingress-192.168.0.1-30080 namespace: mysql2 spec: replicas: 1 selector: matchLabels: run: haproxy-ingress strategy: type: RollingUpdate template: metadata: labels: run: haproxy-ingress spec: tolerations: - key: app operator: Equal value: haproxy effect: NoSchedule serviceAccount: ingress-controller nodeSelector: kubernetes.io/hostname: 192.168.0.1 containers: - args: - --tcp-services-configmap=$(POD_NAMESPACE)/mysql-tcp - --default-backend-service=$(POD_NAMESPACE)/mysql - --default-ssl-certificate=$(POD_NAMESPACE)/tls-secret - --ingress-class=ha-mysql env: - name: POD_NAME valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.namespace image: jcmoraisjr/haproxy-ingress name: haproxy-ingress ports: # 和 configmap 中定义的端口对应 - containerPort: 3306 hostPort: 3306 name: http protocol: TCP - containerPort: 443 name: https protocol: TCP - containerPort: 1936 hostPort: 30081 name: stat protocol: TCP  apiVersion: v1 kind: ServiceAccount metadata: name: ingress-controller --- apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: ingress-controller rules: - apiGroups: - \u0026quot;\u0026quot; resources: - configmaps - pods - secrets - namespaces verbs: - get - apiGroups: - \u0026quot;\u0026quot; resources: - configmaps verbs: - get - update - apiGroups: - \u0026quot;\u0026quot; resources: - configmaps verbs: - create - apiGroups: - \u0026quot;\u0026quot; resources: - endpoints verbs: - get - create - update --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: ingress-controller roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: ingress-controller subjects: - kind: ServiceAccount name: ingress-controller - apiGroup: rbac.authorization.k8s.io kind: User name: ingress-controller --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: ingress-controller rules: - apiGroups: - \u0026quot;\u0026quot; resources: - configmaps - endpoints - nodes - pods - secrets verbs: - list - watch - apiGroups: - \u0026quot;\u0026quot; resources: - nodes verbs: - get - apiGroups: - \u0026quot;\u0026quot; resources: - services verbs: - get - list - watch - apiGroups: - \u0026quot;extensions\u0026quot; resources: - ingresses verbs: - get - list - watch - apiGroups: - \u0026quot;\u0026quot; resources: - events verbs: - create - patch - apiGroups: - \u0026quot;extensions\u0026quot; resources: - ingresses/status verbs: - update --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: ingress-controller roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: ingress-controller subjects: - kind: ServiceAccount name: ingress-controller namespace: mysql2 - apiGroup: rbac.authorization.k8s.io kind: User name: ingress-controller  kubectl apply -f ingress-controller.yaml kubectl apply -f ingress-rbac.yaml -n mysql2  最后创建 ingress 规则：\napiVersion: extensions/v1beta1 kind: Ingress metadata: annotations: ingress.kubernetes.io/ssl-redirect: \u0026quot;false\u0026quot; kubernetes.io/ingress.class: ha-mysql name: ha-mysql spec: rules: - http: paths: - backend: serviceName: mysql-0 servicePort: 3306 path: /  此时可以通过 haproxy 的 IP + 映射端口访问到 mysql 集群。\n附件 以下是上面用到的 yaml 文件：\n- rbac.yaml\n- mysql.yaml\n- secret.yaml\n- pv.yaml\n- mysql-0-svc.yaml\n- tcp-svc.yaml\n- ingress-controller.yaml\n- ingress-rbac.yaml\n- ingress.yaml\n"
},
{
	"uri": "/cloud/sample-controller/",
	"title": "Sample Controller",
	"tags": [],
	"description": "",
	"content": " 自己构建 sample-controller\nhttps://github.com/maoqide/sample-controller https://github.com/kubernetes/sample-controller\n编写 CRD 定义 sample-controller ├── hack │ ├── boilerplate.go.txt │ ├── custom-boilerplate.go.txt │ ├── update-codegen.sh │ └── verify-codegen.sh └── pkg └── apis └── samplecontroller ├── register.go └── v1alpha1 ├── doc.go ├── register.go └── types.go  首先，项目初始如上结构：\nhack目录下的脚本可以复用，主要是调用了 https://github.com/kubernetes/code-generator 项目中的 generate-groups.sh 脚本，code-gengrator 项目 cmd 目录下的代码，需要提前go install生成对应二进制文件。\npkg目录下的文件，需要自己手动编写，pkg/apis/samplecontroller是 CRD 所属的 apiGroup，v1alpha1 是 apiVersion，v1alpha1目录下的types.go文件，包含了 CRD 类型 Foo 的完整定义。\npkg/apis/samplecontroller/register.go中，定义了后面所需的全局变量。\npkg/apis/samplecontroller/v1alpha1/doc.go中，包含了 +\u0026lt;tag-name\u0026gt;[=value] 格式的注释，这就是 Kubernetes 进行源码生成用的 Annotation 风格的注释，doc.go 中的注释，起到的是全局范围的作用，包下面的每个 go 文件，同样可以定义自己的 Annotation 注释。(关于代码生成，可以看这篇文章)\npkg/apis/samplecontroller/v1alpha1/types.go，包含了Foo类型的完整定义。Foo是Kubernetes对象的标准定义；FooSpec是我们需要定义的Foo类型的具体结构；FooList包含一组 Foo 对象，apiserver 的 List 接口，返回的是 List 对象类型；FooStatus描述Foo类型实例状态的结构体，可以使用+genclient:noStatus 注释，则不需要定义FooStatus。\npkg/apis/samplecontroller/v1alpha1/register.go，主要作用是通过addKnownTypes()方法，将我们定义的 CRD 类型 Foo 添加到 Scheme。\n代码生成 pkg下的上述文件完成，即可执行./hack/update-codegen.sh，即可生成管理新定义的 CRD 类型所需的 Kubernetes 代码：\nsample-controller ├── hack │ ├── boilerplate.go.txt │ ├── custom-boilerplate.go.txt │ ├── update-codegen.sh │ └── verify-codegen.sh └── pkg ├── apis │ └── samplecontroller │ ├── register.go │ └── v1alpha1 │ ├── doc.go │ ├── register.go │ ├── types.go │ └── zz_generated.deepcopy.go └── generated ├── clientset │ └── versioned │ ├── clientset.go │ ├── doc.go │ ├── fake │ │ ├── clientset_generated.go │ │ ├── doc.go │ │ └── register.go │ ├── scheme │ │ ├── doc.go │ │ └── register.go │ └── typed │ └── samplecontroller │ └── v1alpha1 │ ├── doc.go │ ├── fake │ │ ├── doc.go │ │ ├── fake_foo.go │ │ └── fake_samplecontroller_client.go │ ├── foo.go │ ├── generated_expansion.go │ └── samplecontroller_client.go ├── informers │ └── externalversions │ ├── factory.go │ ├── generic.go │ ├── internalinterfaces │ │ └── factory_interfaces.go │ └── samplecontroller │ ├── interface.go │ └── v1alpha1 │ ├── foo.go │ └── interface.go └── listers └── samplecontroller └── v1alpha1 ├── expansion_generated.go └── foo.go  自动生成了 clientset，informers，listers 三个文件夹下的文件和apis下的zz_generated.deepcopy.go文件。\n其中zz_generated.deepcopy.go中包含 pkg/apis/samplecontroller/v1alpha1/types.go 中定义的结构体的 DeepCopy() 方法。\n另外三个文件夹clientset，informers，listers下都是 Kubernetes 生成的客户端库，在 controller 中会用到。\ncontroller 代码编写 接下来就是编写具体 controller 的代码，通过上述步骤生成的客户端库访问 apiserver，监听 CRD 资源的变化，并触发对应的动作，如创建或删除 Deployment 等。\n编写自定义controller(Operator)时，可以使用 Kubernetes 提供的 client-go 客户端库。下图是 Kubernetes 提供的在使用client-go开发 controller 的过程中，client-go 和 controller 的交互流程：\nclient-go 组件  Reflector: 定义在 cache 包的 Reflector 类中，它监听特定资源类型(Kind)的 Kubernetes API，在ListAndWatch方法中执行。监听的对象可以是 Kubernetes 的内置资源类型或者是自定义资源类型。当 reflector 通过 watch API 发现新的资源实例被创建，它将通过对应的 list API 获取到新创建的对象并在watchHandler方法中将其加入到Delta Fifo队列中。\n Informer: 定义在 cache 包的 base controller 中，它从Delta Fifo队列中 pop 出对象，在processLoop方法中执行。base controller 的工作是将对象保存一遍后续获取，并调用 controller 将对象传给 controller。\n Indexer: 提供对象的 indexing 方法，定义在 cache 包的 Indexer中。一个典型的 indexing 的应用场景是基于对象的 label 创建索引。Indexer 基于几个 indexing 方法维护索引，它使用线程安全的 data store 来存储对象和他们的key。在 cache 包的 Store 类中定义了一个名为MetaNamespaceKeyFunc的默认方法，可以为对象生成一个\u0026lt;namespace\u0026gt;/\u0026lt;name\u0026gt;形式的key。\n  自定义 controller 组件  Informer reference: 它是对 Informer 实例的引用，知道如何使用自定义资源对象。你编写的自定义 controller 需要创建正确的 Informer。\n Indexer reference: 它是对 Indexer 实例的引用，你编写的自定义 controller 代码中需要创建它，在获取对象供后续使用时你会用到这个引用。  client-go 中的 base controller 提供了NewIndexerInformer来创建 Informer 和 Indexer。在你的代码中，你可以直接使用 此方法，或者使用 工厂方法 创建 informer。\n Resource Event Handlers: 一些回调方法，当 Informer 想要发送一个对象给 controller 时，会调用这些方法。典型的编写回调方法的模式，是获取资源对象的 key 并放入一个 work queue队列，等待进一步的处理(Proceess item)。\n Work queue: 在 controller 代码中创建的队列，用来解耦对象的传递和对应的处理。Resource Event Handlers 的方法就是用来接收对象并将其加入 work queue。\n Process Item: 在 controller 代码中创建的方法，用来对work queue中的对象做对应处理，可以有一个或多个其他的方法实际做处理，这些方法一般会使用Indexer reference，或者 list 方法来获取 key 对应的对象。\n  编写自定义 controller 以 sample-controller 为例，整体流程如下：\n/* *** main.go */ // 创建 clientset kubeClient, err := kubernetes.NewForConfig(cfg)\t// k8s clientset, \u0026quot;k8s.io/client-go/kubernetes\u0026quot; exampleClient, err := clientset.NewForConfig(cfg)\t// sample clientset, \u0026quot;k8s.io/sample-controller/pkg/generated/clientset/versioned\u0026quot; // 创建 Informer kubeInformerFactory := kubeinformers.NewSharedInformerFactory(kubeClient, time.Second*30)\t// k8s informer, \u0026quot;k8s.io/client-go/informers\u0026quot; exampleInformerFactory := informers.NewSharedInformerFactory(exampleClient, time.Second*30)\t// sample informer, \u0026quot;k8s.io/sample-controller/pkg/generated/informers/externalversions\u0026quot; // 创建 controller，传入 clientset 和 informer controller := NewController(kubeClient, exampleClient, kubeInformerFactory.Apps().V1().Deployments(), exampleInformerFactory.Samplecontroller().V1alpha1().Foos()) // 运行 Informer，Start 方法为非阻塞，会运行在单独的 goroutine 中 kubeInformerFactory.Start(stopCh)\texampleInformerFactory.Start(stopCh) // 运行 controller controller.Run(2, stopCh) /* *** controller.go */ NewController() *Controller {} // 将 CRD 资源类型定义加入到 Kubernetes 的 Scheme 中，以便 Events 可以记录 CRD 的事件 utilruntime.Must(samplescheme.AddToScheme(scheme.Scheme)) // 创建 Broadcaster eventBroadcaster := record.NewBroadcaster() // ... ... // 监听 CRD 类型'Foo'并注册 ResourceEventHandler 方法，当'Foo'的实例变化时进行处理 fooInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ AddFunc: controller.enqueueFoo, UpdateFunc: func(old, new interface{}) { controller.enqueueFoo(new) }, }) // 监听 Deployment 变化并注册 ResourceEventHandler 方法， // 当它的 ownerReferences 为 Foo 类型实例时，将该 Foo 资源加入 work queue deploymentInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ AddFunc: controller.handleObject, UpdateFunc: func(old, new interface{}) { newDepl := new.(*appsv1.Deployment) oldDepl := old.(*appsv1.Deployment) if newDepl.ResourceVersion == oldDepl.ResourceVersion { return } controller.handleObject(new) }, DeleteFunc: controller.handleObject, }) func (c *Controller) Run(threadiness int, stopCh \u0026lt;-chan struct{}) error {} // 在启动 worker 前等待缓存同步 if ok := cache.WaitForCacheSync(stopCh, c.deploymentsSynced, c.foosSynced); !ok { return fmt.Errorf(\u0026quot;failed to wait for caches to sync\u0026quot;) } // 运行两个 worker 来处理资源 for i := 0; i \u0026lt; threadiness; i++ { go wait.Until(c.runWorker, time.Second, stopCh) } // 无限循环，不断的调用 processNextWorkItem 处理下一个对象 func (c *Controller) runWorker() { for c.processNextWorkItem() { } } // 从workqueue中获取下一个对象并进行处理，通过调用 syncHandler func (c *Controller) processNextWorkItem() bool { obj, shutdown := c.workqueue.Get() if shutdown { return false } err := func(obj interface{}) error { // 调用 workqueue.Done(obj) 方法告诉 workqueue 当前项已经处理完毕， // 如果我们不想让当前项重新入队，一定要调用 workqueue.Forget(obj)。 // 当我们没有调用Forget时，当前项会重新入队 workqueue 并在一段时间后重新被获取。 defer c.workqueue.Done(obj) var key string var ok bool // 我们期望的是 key 'namespace/name' 格式的 string if key, ok = obj.(string); !ok { // 无效的项调用Forget方法，避免重新入队。 c.workqueue.Forget(obj) utilruntime.HandleError(fmt.Errorf(\u0026quot;expected string in workqueue but got %#v\u0026quot;, obj)) return nil } if err := c.syncHandler(key); err != nil { // 放回workqueue避免偶发的异常 c.workqueue.AddRateLimited(key) return fmt.Errorf(\u0026quot;error syncing '%s': %s, requeuing\u0026quot;, key, err.Error()) } // 如果没有异常，Forget当前项，同步成功 c.workqueue.Forget(obj) klog.Infof(\u0026quot;Successfully synced '%s'\u0026quot;, key) return nil }(obj) if err != nil { utilruntime.HandleError(err) return true } return true } // 对比真实的状态和期望的状态并尝试合并，然后更新Foo类型实例的状态信息 func (c *Controller) syncHandler(key string) error { // 通过 workqueue 中的 key 解析出 namespace 和 name namespace, name, err := cache.SplitMetaNamespaceKey(key) // 调用 lister 接口通过 namespace 和 name 获取 Foo 实例 foo, err := c.foosLister.Foos(namespace).Get(name) deploymentName := foo.Spec.DeploymentName // 获取 Foo 实例中定义的 deploymentname deployment, err := c.deploymentsLister.Deployments(foo.Namespace).Get(deploymentName) // 没有发现对应的 deployment，新建一个 if errors.IsNotFound(err) { deployment, err = c.kubeclientset.AppsV1().Deployments(foo.Namespace).Create(newDeployment(foo)) } // OwnerReferences 不是 Foo 实例，warning并返回错误 if !metav1.IsControlledBy(deployment, foo) { msg := fmt.Sprintf(MessageResourceExists, deployment.Name) c.recorder.Event(foo, corev1.EventTypeWarning, ErrResourceExists, msg) return fmt.Errorf(msg) } // deployment 中 的配置和 Foo 实例中 Spec 的配置不一致，即更新 deployment if foo.Spec.Replicas != nil \u0026amp;\u0026amp; *foo.Spec.Replicas != *deployment.Spec.Replicas { deployment, err = c.kubeclientset.AppsV1().Deployments(foo.Namespace).Update(newDeployment(foo)) } // 更新 Foo 实例状态 err = c.updateFooStatus(foo, deployment) c.recorder.Event(foo, corev1.EventTypeNormal, SuccessSynced, MessageResourceSynced) }  接下来编写对应的 CRD 和 对应 CRD 实例的 yaml 文件及 operator 的 Dockerfile：\nsample-controller ├── artifacts │ └── examples │ ├── crd.yaml │ └── example-foo.yaml ├── controller.go ├── Dockerfile ├── hack │ ├── boilerplate.go.txt │ ├── custom-boilerplate.go.txt │ ├── update-codegen.sh │ └── verify-codegen.sh ├── main.go └── pkg ├── apis │ └── samplecontroller │ ├── register.go │ └── v1alpha1 │ ├── doc.go │ ├── register.go │ ├── types.go │ └── zz_generated.deepcopy.go ├── generated │ ├── clientset │ │ └── ... │ ├── informers │ │ └── ... │ └── listers │ └── ... └── signals └── signal.go  部署到 k8s controller 镜像 Dockerfile:\nFROM golang RUN mkdir -p /go/src/k8s.io/sample-controller ADD . /go/src/k8s.io/sample-controller WORKDIR /go RUN go get -v ./... RUN go install -v ./... CMD [\u0026quot;/go/bin/sample-controller\u0026quot;]  controller RBAC 及 Deployment yaml:\napiVersion: apps/v1beta1 kind: Deployment metadata: name: sample-controller spec: replicas: 1 template: metadata: labels: app: sample spec: containers: - name: sample image: \u0026quot;maoqide/sample-controller\u0026quot; --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: operator-role rules: - apiGroups: - \u0026quot;\u0026quot; resources: - events verbs: - get - list - watch - create - update - patch - delete - apiGroups: - apps resources: - deployments - events verbs: - get - list - watch - create - update - patch - delete - apiGroups: - samplecontroller.k8s.io resources: - foos verbs: - get - list - watch - create - update - patch - delete --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: operator-rolebinding roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: operator-role subjects: - kind: ServiceAccount name: default namespace: default  将 operator 部署到 k8s 中并创建一个 CRD 对象，即可看到 operator 自动按照 CRD 对象 的配置创建出一个 nginx Deployment。\n"
},
{
	"uri": "/cloud/building-an-operator-for-kubernetes-with-the-sample-controller/",
	"title": "Building an Operator for Kubernetes With the Sample Controller",
	"tags": [],
	"description": "",
	"content": " An Operator is an application-specific controller that extends the Kubernetes API to create, configure, and manage instances of complex stateful applications on behalf of a Kubernetes user.  Operator 是一个特定的应用程序的控制器，通过扩展 Kubernetes API 以代表 Kubernetes 用户创建，配置和管理复杂有状态应用程序的实例。\nOperator 是一种软件，它结合了特定的领域知识并通过 CRD(Custom Resource Definition ) 机制扩展了Kubernetes API，使用户像管理 Kubernetes 的内置资源一样创建，配置和管理应用程序。Operator 管理整个集群中的多个实例，而不仅仅管理应用程序的单个实例。\n[译] https://itnext.io/building-an-operator-for-kubernetes-with-the-sample-controller-b4204be9ad56\nThe sample-controller 创建示例的 operator 程序需要用到的第一个工具是 sample-controller, 可以在 https://github.com/kubernetes/sample-controller 找到。\n这个项目实现了一个简单的 Foo 类型的 operator, 当创建一个自定义类型的对象 foo，operator 会创建一个 以几个公开的 docker 镜像和特定的副本数创建一个 Deployment。\n要安装和编译它，需要确认你的 GOPATH，然后执行:\ngo get github.com/kubernetes/sample-controller cd $GOPATH/src/k8s.io/sample-controller go build -o ctrl .  接着我们可以用artifacts/examples目录下的文件，创建Foo类型的自定义资源定义(CRD)。\nkubectl apply -f artifacts/examples/crd-validation.yaml  现在从另一个终端，我们可以操作Foo对象并观察 controller 发生了什么：\n$ kubectl apply -f artifacts/examples/example-foo.yaml $ kubectl get pods NAME READY STATUS RESTARTS AGE example-foo-6cbc69bf5d-j8lhx 1/1 Running 0 18s $ kubectl delete -f artifacts/examples/example-foo.yaml $ kubectl get pods NAME READY STATUS RESTARTS AGE example-foo-6cbc69bf5d-j8lhx 0/1 Terminating 0 38s  在 Kubernetes 1.11.0，controller 会进入无限循环，当 foo 对象创建一个 deployment 后更新它的状态：在`updateFooStatus`方法中，你必调用`UpdateStatus(fooCopy)`代替`Update(fooCopy)`。  到目前为止，控制器完成了这样一项工作：它在我们创建foo对象时创建一个deployment并在我们删除对象时停止deployment。\n现在我们可以进一步调整 CRD 和 controller 以使用我们自己的自定义资源定义。\nAdapting the sample-controller 假设我们的目标是编写一个在集群节点上部署守护程序的 operator。它会使用 DaemonSet 对象来部署此守护程序，并且能够指定标签，仅在打上此标签的节点上部署守护程序。我们，还希望能够指定部署的 docker 镜像，而不是像sample-controller的例子那样静态的。\n我们首先为GenericDaemon类型创建自定义资源定义：\n// artifacts/generic-daemon/crd.yaml apiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: name: genericdaemons.mydomain.com spec: group: mydomain.com version: v1beta1 names: kind: Genericdaemon plural: genericdaemons scope: Namespaced validation: openAPIV3Schema: properties: spec: properties: label: type: string image: type: string required: - image  以及第一个要部署的守护程序的示例：\n// artifacts/generic-daemon/syslog.yaml apiVersion: mydomain.com/v1beta1 kind: Genericdaemon metadata: name: syslog spec: label: logs image: mbessler/syslogdocker  现在我们必须为 operator 访问新的自定义资源定义(CRD)的 API 构建 go 文件。为此，我们要创建一个新的目录pkg/apis/genericdaemon，在这个目录中复制pkg/apis/samplecontroller目录下的文件(除了zz_generated.deepcopy.go)。\n$ tree pkg/apis/genericdaemon/ pkg/apis/genericdaemon/ ├── register.go └── v1beta1 ├── doc.go ├── register.go └── types.go  并调整其内容：\n//////////////// // register.go //////////////// package genericdaemon const ( GroupName = \u0026quot;mydomain.com\u0026quot; ) ///////////////////// // v1beta1/doc.go ///////////////////// // +k8s:deepcopy-gen=package // Package v1beta1 is the v1beta1 version of the API. // +groupName=mydomain.com package v1beta1 ///////////////////////// // v1beta1/register.go ///////////////////////// package v1beta1 import ( metav1 \u0026quot;k8s.io/apimachinery/pkg/apis/meta/v1\u0026quot; \u0026quot;k8s.io/apimachinery/pkg/runtime\u0026quot; \u0026quot;k8s.io/apimachinery/pkg/runtime/schema\u0026quot; genericdaemon \u0026quot;k8s.io/sample-controller/pkg/apis/genericdaemon\u0026quot; ) // SchemeGroupVersion is group version used to register these objects var SchemeGroupVersion = schema.GroupVersion{Group: genericdaemon.GroupName, Version: \u0026quot;v1beta1\u0026quot;} // Kind takes an unqualified kind and returns back a Group qualified GroupKind func Kind(kind string) schema.GroupKind { return SchemeGroupVersion.WithKind(kind).GroupKind() } // Resource takes an unqualified resource and returns a Group qualified GroupResource func Resource(resource string) schema.GroupResource { return SchemeGroupVersion.WithResource(resource).GroupResource() } var ( SchemeBuilder = runtime.NewSchemeBuilder(addKnownTypes) AddToScheme = SchemeBuilder.AddToScheme ) // Adds the list of known types to Scheme. func addKnownTypes(scheme *runtime.Scheme) error { scheme.AddKnownTypes(SchemeGroupVersion, \u0026amp;Genericdaemon{}, \u0026amp;GenericdaemonList{}, ) metav1.AddToGroupVersion(scheme, SchemeGroupVersion) return nil } ////////////////////// // v1beta1/types.go ////////////////////// package v1beta1 import ( metav1 \u0026quot;k8s.io/apimachinery/pkg/apis/meta/v1\u0026quot; ) // +genclient // +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object // Genericdaemon is a specification for a Generic Daemon resource type Genericdaemon struct { metav1.TypeMeta `json:\u0026quot;,inline\u0026quot;` metav1.ObjectMeta `json:\u0026quot;metadata,omitempty\u0026quot;` Spec GenericdaemonSpec `json:\u0026quot;spec\u0026quot;` Status GenericdaemonStatus `json:\u0026quot;status\u0026quot;` } // GenericDaemonSpec is the spec for a GenericDaemon resource type GenericdaemonSpec struct { Label string `json:\u0026quot;label\u0026quot;` Image string `json:\u0026quot;image\u0026quot;` } // GenericDaemonStatus is the status for a GenericDaemon resource type GenericdaemonStatus struct { Installed int32 `json:\u0026quot;installed\u0026quot;` } // +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object // GenericDaemonList is a list of GenericDaemon resources type GenericdaemonList struct { metav1.TypeMeta `json:\u0026quot;,inline\u0026quot;` metav1.ListMeta `json:\u0026quot;metadata\u0026quot;` Items []Genericdaemon `json:\u0026quot;items\u0026quot;` }  脚本hack/update-codegen.sh可用于生成我们之前文件中定义的新的自定义资源定义(CRD)的代码，我们必需修改此脚本来为我们的新 CRD 生成文件：\n# hack/update-codegen.sh #!/usr/bin/env bash set -o errexit set -o nounset set -o pipefail SCRIPT_ROOT=$(dirname ${BASH_SOURCE})/.. CODEGEN_PKG=${CODEGEN_PKG:-$(cd ${SCRIPT_ROOT}; ls -d -1 ./vendor/k8s.io/code-generator 2\u0026gt;/dev/null || echo ../code-generator)} # generate the code with: # --output-base because this script should also be able to run inside the vendor dir of # k8s.io/kubernetes. The output-base is needed for the generators to output into the vendor dir # instead of the $GOPATH directly. For normal projects this can be dropped. ${CODEGEN_PKG}/generate-groups.sh \u0026quot;deepcopy,client,informer,lister\u0026quot; \\ k8s.io/sample-controller/pkg/client k8s.io/sample-controller/pkg/apis \\ genericdaemon:v1beta1 \\ --output-base \u0026quot;$(dirname ${BASH_SOURCE})/../../..\u0026quot; \\ --go-header-file ${SCRIPT_ROOT}/hack/boilerplate.go.txt  接着执行此脚本：\n$ ./hack/update-codegen.sh Generating deepcopy funcs Generating clientset for genericdaemon:v1beta1 at k8s.io/sample-controller/pkg/client/clientset Generating listers for genericdaemon:v1beta1 at k8s.io/sample-controller/pkg/client/listers Generating informers for genericdaemon:v1beta1 at k8s.io/sample-controller/pkg/client/informers  现在可以调整它来编写我们的 operator。首先，我们必须将所有对之前的Foo类型的引用修改为Genericdaemon类型。另外，当一个新的 genericdaemon实例创建后，我们要创建 DaemonSet 而不是 Deployment。\nDeploying the operator to the Kubernetes cluster 当我们将sample-controller修改为我们需要的之后，我们要将它部署到kubernetes集群。事实上，在这个时候，我们已经使用我们的凭证将它运行在我们的开发系统来测试它。\n这是一个简单的 Dockerfile，用于构建 operator 的 Docker 镜像（你必须删除原有的sample-controller中的所有代码才能构建）：\nFROM golang RUN mkdir -p /go/src/k8s.io/sample-controller ADD . /go/src/k8s.io/sample-controller WORKDIR /go RUN go get ./... RUN go install -v ./... CMD [\u0026quot;/go/bin/sample-controller\u0026quot;]  现在我们可以构建并将镜像推送到 DockerHub：\ndocker build . -t mydockerid/genericdaemon docker push mydockerid/genericdaemon  最后用这个新的镜像部署一个 Deployment：\n// deploy.yaml apiVersion: apps/v1beta1 kind: Deployment metadata: name: sample-controller spec: replicas: 1 selector: matchLabels: app: sample template: metadata: labels: app: sample spec: containers: - name: sample image: \u0026quot;mydockerid/genericdaemon:latest\u0026quot;  并kubectl apply -f deploy.yaml。\noperator 现在已经运行，但是如果我们查看 pod 的日志，可以看到授权存在问题; pod 没有对不同资源的访问权限：\n$ kubectl logs sample-controller-66b79c7d5f-2qnft E0721 14:34:50.499584 1 reflector.go:134] k8s.io/sample-controller/pkg/client/informers/externalversions/factory.go:117: Failed to list *v1beta1.Genericdaemon: genericdaemons.mydomain.com is forbidden: User \u0026quot;system:serviceaccount:default:default\u0026quot; cannot list genericdaemons.mydomain.com at the cluster scope E0721 14:34:50.500385 1 reflector.go:134] k8s.io/client-go/informers/factory.go:131: Failed to list *v1.DaemonSet: daemonsets.apps is forbidden: User \u0026quot;system:serviceaccount:default:default\u0026quot; cannot list daemonsets.apps at the cluster scope [...]  我们需要创建一个ClusterRole和一个ClusterRoleBinding来为 operator 提供必要的权限：\n// rbac_role.yaml kind: ClusterRole metadata: name: operator-role rules: - apiGroups: - apps resources: - daemonsets verbs: - get - list - watch - create - update - patch - delete - apiGroups: - mydomain.com resources: - genericdaemons verbs: - get - list - watch - create - update - patch - delete // rbac_role_binding.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: operator-rolebinding roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: operator-role subjects: - kind: ServiceAccount name: default namespace: default  并且部署它：\nkubectl apply -f rbac_role.yaml kubectl delete -f deploy.yaml kubectl apply -f deploy.yaml  现在，你的 operator 应该已经部署到你的 Kubernetes 集群并处于活动状态。\n"
},
{
	"uri": "/golang/golang-mutex/",
	"title": "Golang Mutex",
	"tags": [],
	"description": "",
	"content": " golang 的sync包中有两种锁，互斥锁sync.Mutex 和读写锁sync.RWMutex。\nsync.Mutex  Mutex 为互斥锁，Lock() 加锁，Unlock() 解锁\n 使用 Lock() 加锁后，在使用Unlock()解锁前便不能再次对其进行加锁，否则会导致死锁\n 在Lock()前使用Unlock()会导致 panic 异常\n 适用于读写不确定场景，即读写次数没有明显的区别，并且只允许只有一个读或者写的场景\n  sync.RWMutex  RWMutex 是单写多读锁，可以加多个读锁或者一个写锁\n 读锁占用的情况下会阻止写，不会阻止读，多个 goroutine 可以同时获取读锁\n 写锁会阻止其他 goroutine（无论读和写）进来，整个锁由该 goroutine 独占\n 适用于读多写少的场景\n  Lock()/Unlock()  Lock() 加写锁，Unlock() 解写锁\n 写锁权限高于读锁，有写锁时优先加写锁\n 在 Lock() 之前使用 Unlock() 会导致 panic 异常\n  RLock()/RUnlock()  RLock() 加读锁，RUnlock() 解读锁\n RLock() 加读锁时，如果存在写锁，则无法加读锁；当只有读锁或者没有锁时，可以加读锁，读锁可以加多个\n RUnlock() 解读锁，RUnlock() 撤销单次 RLock() 调用，对于其他同时存在的读锁没有作用\n 不能在没有读锁的情况下调用RUnlock()，RUnlock()不得多于RLock()，否则会导致 panic 异常\n参考：https://blog.csdn.net/chenbaoke/article/details/41957725\n  "
},
{
	"uri": "/cloud/kube-scheduler-%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/",
	"title": "Kube Scheduler 源码阅读",
	"tags": [],
	"description": "",
	"content": "NewSchedulerCommand() runCommand() opts.Config()\t// create client \u0026amp; informer func Run(cc schedulerserverconfig.CompletedConfig, stopCh \u0026lt;-chan struct{}) error {} run := func(ctx context.Context) {} sched.Run() // Run begins watching and scheduling. It waits for cache to be synced, then starts a goroutine and returns immediately. func (sched *Scheduler) Run() {} go wait.Until(sched.scheduleOne, 0, sched.config.StopEverything) // scheduleOne does the entire scheduling workflow for a single pod. func (sched *Scheduler) scheduleOne() {} pod := sched.config.NextPod() sched.schedule(pod) func (sched *Scheduler) schedule(pod *v1.Pod) (string, error) {} host, err := sched.config.Algorithm.Schedule(pod, sched.config.NodeLister) // Schedule tries to schedule the given pod to one of the nodes in the node list. // If it succeeds, it will return the name of the node. // If it fails, it will return a FitError error with reasons. // generic algorithm func (g *genericScheduler) Schedule(pod *v1.Pod, nodeLister algorithm.NodeLister) (string, error) {} // podPassesBasicChecks makes sanity checks on the pod if it can be scheduled. // 检查 pod 是否使用 pvc，且 pvc 是否可用 podPassesBasicChecks(pod, g.pvcLister) // Computing predicates, 并发筛选符合 predicates 的节点 filteredNodes, failedPredicateMap, err := g.findNodesThatFit(pod, nodes) // Filters the nodes to find the ones that fit based on the given predicate functions // Each node is passed through the predicate functions to determine if it is a fit func (g *genericScheduler) findNodesThatFit(pod *v1.Pod, nodes []*v1.Node) ([]*v1.Node, FailedPredicateMap, error) {} fits, failedPredicates, err := podFitsOnNode(pod, meta, g.cachedNodeInfoMap[nodeName], g.predicates, g.schedulingQueue, g.alwaysCheckAllPredicates,) workqueue.ParallelizeUntil(ctx, 16, int(allNodes), checkNode) // podFitsOnNode checks whether a node given by NodeInfo satisfies the given predicate functions. // For given pod, podFitsOnNode will check if any equivalent pod exists and try to reuse its cached // predicate results as possible. // This function is called from two different places: Schedule and Preempt. // When it is called from Schedule, we want to test whether the pod is schedulable // on the node with all the existing pods on the node plus higher and equal priority // pods nominated to run on the node. // When it is called from Preempt, we should remove the victims of preemption and // add the nominated pods. Removal of the victims is done by SelectVictimsOnNode(). // It removes victims from meta and NodeInfo before calling this function. func podFitsOnNode( pod *v1.Pod, meta predicates.PredicateMetadata, info *schedulernodeinfo.NodeInfo, predicateFuncs map[string]predicates.FitPredicate, queue internalqueue.SchedulingQueue, alwaysCheckAllPredicates bool, ) (bool, []predicates.PredicateFailureReason, error) { // Prioritizing, priorityList, err := PrioritizeNodes(pod, g.cachedNodeInfoMap, metaPrioritiesInterface, g.prioritizers, filteredNodes, g.extenders) // PrioritizeNodes prioritizes the nodes by running the individual priority functions in parallel. // Each priority function is expected to set a score of 0-10 // 0 is the lowest priority score (least preferred node) and 10 is the highest // Each priority function can also have its own weight // The node scores returned by the priority function are multiplied by the weights to get weighted scores // All scores are finally combined (added) to get the total weighted scores of all nodes func PrioritizeNodes( pod *v1.Pod, nodeNameToInfo map[string]*schedulernodeinfo.NodeInfo, meta interface{}, priorityConfigs []algorithm.PriorityConfig, nodes []*v1.Node, extenders []algorithm.SchedulerExtender, ) (schedulerapi.HostPriorityList, error) {} // PrioritizeNodes prioritizes the nodes by running the individual priority functions in parallel. // Each priority function is expected to set a score of 0-10 // 0 is the lowest priority score (least preferred node) and 10 is the highest // Each priority function can also have its own weight // The node scores returned by the priority function are multiplied by the weights to get weighted scores // All scores are finally combined (added) to get the total weighted scores of all nodes func PrioritizeNodes( pod *v1.Pod, nodeNameToInfo map[string]*schedulernodeinfo.NodeInfo, meta interface{}, priorityConfigs []algorithm.PriorityConfig, nodes []*v1.Node, extenders []algorithm.SchedulerExtender, ) (schedulerapi.HostPriorityList, error) {} // Selecting host g.selectHost(priorityList) // selectHost takes a prioritized list of nodes and then picks one // in a round-robin manner from the nodes that had the highest score. func (g *genericScheduler) selectHost(priorityList schedulerapi.HostPriorityList) (string, error) {}   Predicate 算法：pkg\\scheduler\\algorithm\\predicates\n Priority 算法：pkg\\scheduler\\algorithm\\priorities  "
},
{
	"uri": "/collection/collection-2/",
	"title": "Collection 2",
	"tags": [],
	"description": "",
	"content": " goroutine 调度器解析。\nhttps://tonybai.com/2017/06/23/an-intro-about-goroutine-scheduler/\n备用链接\n Golang for range性能分析及优化: for range 会有元素拷贝操作，造成性能方面的影响。\ngolang for range\n备用链接\n 一致性哈希算法。\nhttps://segmentfault.com/a/1190000013533592\n备用链接\n ETCD的一些使用场景及原理分析。 https://infoq.cn/article/etcd-interpretation-application-scenario-implement-principle 备用链接\n ETCD Raft 算法解析。 https://draveness.me/etcd-introduction\n备用链接\n  "
},
{
	"uri": "/cloud/container-network/",
	"title": "Container Network",
	"tags": [],
	"description": "",
	"content": " 参考及极客时间 https://time.geekbang.org/column/article/65287\n同主机 Veth Pair\n如上图，同主机容器，通过容器启动时创建的一对 Veth Pair 设备进行外部通信，Veth Pair 设备一端插在宿主机 docker0网桥上，一端是容器内的eth0网卡, 在宿主机上执行brctl show docker0可以查看连到docker0网桥上的设备。\n容器内默认的路由表：\nbash-4.3# route Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface default 172.17.0.1 0.0.0.0 UG 0 0 0 eth0 172.17.0.0 * 255.255.0.0 U 0 0 0 eth0  当同一主机上的容器 container1 需要和 container2 通信时，根据路由规则，container1 通过 eth0 网卡发往 container2 地址，两容器 IP 处于同一网段，container1 通过 ARP 协议获得 container2 MAC 地址，就可以将消息发送出去。\n跨主机 Flannel  UDP\n VXLAN\n host-gw\n  UDP UDP 模式的是性能最差的方式，目前已弃用。(因为在通信过程中要进行多次用户态到内核态的切换)\n如上图，flannel 会在每台宿主机上创建一系列的路由规则，一个 TUN 设备 flannel0(TUN 设备是一种工作在三层的虚拟网络设备，可以在内核和用户应用程序间传递IP包)，以及启动 flanneld 进程，监听 8285 端口。\nflannel 还会为 flannel 网络内的每台主机分配一个 subnet，容器会从所在宿主机的 subnet 中获取 IP。subnet 和 宿主机的对应关系保存在 etcd 中。\n当 node1 上的容器 container1 发送数据给同一 flannel 网络内节点 node2 上的容器 container2 时，因为两容器 IP 不在同一网段，首先会通过容器网关进入 docker0 网桥到达宿主机，然后又根据 flannel 创建的路由规则进入到 flannel0，并由 flannel0 转给 flanneld 进程， flannel 会根据目的容器 container2 的 IP 匹配到它对应的 subnet(100.96.2.0/24)，并从 etcd 中通过 subnet 获取到对应宿主机 node2 的 IP，这样，node1 上的 flanneld 进程就可以将数据发送给 node2上 的 flanneld 进程，进而将数据发送到目的容器。\n$ ip route default via 10.168.0.1 dev eth0 # 非本机 subnet 下的 flannel 网络范围内的地址默认匹配到 flannel0 100.96.0.0/16 dev flannel0 proto kernel scope link src 100.96.1.0 # 本机 subnet 下的地址匹配到 docker0 100.96.1.0/24 dev docker0 proto kernel scope link src 100.96.1.1 10.168.0.0/24 dev eth0 proto kernel scope link src 10.168.0.2  UDP模式下，docker0 网桥的地址范围必须是 flannel 为宿主机分配的子网。\nVXLAN VXLAN 模式封包解包都是在内核态中进行，所以和 UDP 模式相比性能较好。 vxlan 模式下，flannel 会在每台宿主机上创建路由规则，一个 VTEP(虚拟隧道端点) 设备 flannel.1。在通信过程中，封包解包操作直接由 VTEP 在内核态完成，而不用 flanneld 进行处理，这些 VTEP 间组成一个虚拟的二层网络，通过二层数据帧进行通信。\n当 node1 上的容器 container1 发送数据给同一 flannel 网络内节点 node2 上的容器 container2 时，同样先通过 docker0 到达宿主机 node1，然后会被转发到本机 VTEP 设备 flannel.1 进行处理；当一个节点加入 flannel 后，其他所有节点的 flanneld 进程都会在本地添加一条路由规则，即发往目的节点对应网段的数据包，都要经过 flannel.1 最终发往目的节点的 VTEP 设备的 IP 地址。根据这条规则，node1 的 flannel.1 设备收到数据包后，就要获 node2 的 VTEP 设备的 MAC 地址，将数据包封装成二层数据帧。为了可以获取 VTEP 的 MAC 地址，flanneld 进程会在新的节点加入后，在所有节点添加一条 ARP 记录，记录新节点 IP 对应的 MAC 地址，这样，Linux 内核就可以通过 ARP 记录直接获取到目的节点的 VTEP 设备的 MAC 地址并封装成数据帧(内部数据帧)，为了能够在宿主机网络传输，Linux 内核会再将此数据帧加上一个 VXLAN 头封装成外部数据帧。\n$ route -n Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface ... # flannel 当 node2 启动，其他所有节点会添加如下规则，即发往 10.1.16.0/24 网段的数据包， # 都要经过 flannel.1, 且最终发往的网关地址为 10.1.16.0, 即 node2 上的 VTEP 设备 IP 地址。 10.1.16.0 10.1.16.0 255.255.255.0 UG 0 0 0 flannel.1  为了数据包能从 node1 的 flannel.1 设备发送到 node2 的flannel.1 设备上，还是需要借助宿主机网络从 node1 的 eth0 网卡发出到 node2 的 eth0 网卡。如何知道目的 VTEP 设备的宿主机地址呢? 此时 flannel.1 设备充当网桥的角色，在二层网络进行 UDP 包的转发。并由 flanneld 负责维护 falnnel.1 对应的 FDB 信息，通过 FDB 记录查询网桥转发的规则，这样就找到了 UDP 包要被发往的宿主机地址，接下来就可以通过正常的发包流程将数据发出去了。\n# 在 Node 1 上，使用“目的 VTEP 设备”的 MAC 地址进行查询 $ bridge fdb show flannel.1 | grep 5e:f8:4f:00:e3:37 5e:f8:4f:00:e3:37 dev flannel.1 dst 10.168.0.3 self permanent  host-gw host-gw 模式下，flanneld 在每台宿主机上创建路由规则，将每个 flannel 子网的下一跳，设置成该子网对应的宿主机的 IP 地址。也就是说，这台宿主机会充当这条容器通信路径中的*网关*。Flannel 子网和主机的信息，都保存在 ETCD 中，由 flanneld 监听 ETCD 中的数据变化，并实时更新路由表。\nhost-gw 模式要求宿主机直接二层连通。\n"
},
{
	"uri": "/cloud/docker-%E5%8E%9F%E7%90%86/",
	"title": "Docker 原理",
	"tags": [],
	"description": "",
	"content": "  Namespace Cgroup UnionFS  Namespace    Namespace 隔离内容     UTS 主机名与域名   IPC 信号量、消息队列和共享内存   PID 进程编号   Network 网络设备、网络栈、端口等   Mount 挂载点（文件系统）   User 用户和用户组    Cgroup  blkio: 为块设备设定输入输出/限制，比如物理驱动设备（包括磁盘、固态硬盘、USB等）\n cpu: 使用调度程序控制任务对 CPU 的使用\n cpuacct: 自动生成 cgroup 中任务对 CPU 资源使用情况的报告\n cpuset: 为 cgroup 中的任务分配独立的 CPU 和内存\n devices: 可以开启或关闭 cgroup 中任务对设备的访问\n freezer: 可以挂起或恢复 cgroup 中的任务\n memory: 可以设定 cgroup 中任务对内存使用量的限定，并且自动生成这些任务对内存资源使用情况的报告\n perf_event: 使用后使 cgroup 中的任务可以进行统一的性能测试\n net_cls: Docker 没有直接使用它，它通过使用等级识别符（classid）标记网络数据包，从而允许 Linux 流量控制程序识别从具体 cgroup 中生成的数据包\n  子系统文件 公共  tasks: 这个文件罗列所有该 cgroup 中任务的 TID，即所有线程或进程的 ID\n cgroup.procs: 这个文件罗列所有该 cgroup 中的线程组ID(TGID), 及线程组中第一个进程的ID\n notify_on_release: 0或1，表示是否在 cgroup 中最后一个任务退出时通知运行 release agent, 默认0, 表示不运行\n release_agent: 指定 release agent 执行脚本的文件路径，这个脚本通常用于自动化卸载无用的 cgroup\n  cpu cpu子系统根据进程设置的调度属性，选择对应的CPU资源调度方法。\n1. 完全公平调度 Completely Fair Scheduler (CFS)\n限上限，cpu.cfs_period_us, cpu.cfs_quota_us\ncpu.cfs_period_us = 统计CPU使用时间的周期\ncpu.cfs_quota_us = 周期内允许占用的CPU时间(指单核的时间, 多核则需要在设置时累加)\nCFS 用于处理以下几种进程调度策略:\n- SCHED_OTHER\n- SCHED_BATCH\n- SCHED_IDLE\ncfs_period_us用来配置时间周期长度, cfs_quota_us用来配置当前 cgroup 在设置的周期长度内所能使用的 CPU 时间数，两个文件配合起来设置 CPU 的使用上限。两个文件的单位都是微秒（us），cfs_period_us的取值范围为1毫秒（ms）到1秒（s），cfs_quota_us的取值大于 1ms 即可，如果 cfs_quota_us 的值为 -1（默认值），表示不受 cpu 时间的限制。\n例：\n#设置只能使用1个cpu的20%的时间 echo 50000 \u0026gt; cpu.cfs_period_us echo 10000 \u0026gt; cpu.cfs_quota_us #设置完全使用4个cpu的时间 echo 1000000 \u0026gt; cpu.cfs_period_us echo 4000000 \u0026gt; cpu.cfs_quota_us   实时调度 Real-Time scheduler (RT)\n限实时任务上限，cpu.rt_period_us，cpu.rt_runtime_us\ncpu.rt_period_us = 统计CPU使用时间的周期\ncpu.rt_runtime_us = 周期内允许任务使用单个CPU核的时间，如果系统中有多个核，则可以使用核倍数的时间 (计算方法与cfs不一样，需要注意)\n  RT用于处理以下几种进程调度策略\n- SCHED_FIFO\n- SCHED_RR\n cpu.shares\nshares用来设置CPU的相对值，并且是针对所有的CPU（内核），默认值是1024。\n假如系统中有两个cgroup，分别是A和B，A的shares值是1024，B的shares值是512，那么A将获得1024/(1204+512)=66%的CPU资源，而B将获得33%的CPU资源。 shares有两个特点：\n 如果A不忙，没有使用到66%的CPU时间，那么剩余的CPU时间将会被系统分配给B，即B的CPU使用率可以超过33%\n 如果添加了一个新的cgroup C，且它的shares值是1024，那么A的限额变成了1024/(1204+512+1024)=40%，B的变成了20%\n cpu.stat\n包含了下面三项统计结果：\nnr_periods： 表示过去了多少个cpu.cfs_period_us里面配置的时间周期\nnr_throttled： 在上面的这些周期中，有多少次是受到了限制（即cgroup中的进程在指定的时间周期中用光了它的配额）\nthrottled_time: cgroup中的进程被限制使用CPU持续了多长时间(纳秒)\n  memory cgroup.event_control #用于eventfd的接口 memory.usage_in_bytes #显示当前已用的内存 memory.limit_in_bytes #设置/显示当前限制的内存额度 memory.failcnt #显示内存使用量达到限制值的次数 memory.max_usage_in_bytes #历史内存最大使用量 memory.soft_limit_in_bytes #设置/显示当前限制的内存软额度 memory.stat #显示当前cgroup的内存使用情况 memory.use_hierarchy #设置/显示是否将子cgroup的内存使用情况统计到当前cgroup里面 memory.force_empty #触发系统立即尽可能的回收当前cgroup中可以回收的内存 memory.pressure_level #设置内存压力的通知事件，配合cgroup.event_control一起使用 memory.swappiness #设置和显示当前的swappiness memory.move_charge_at_immigrate #设置当进程移动到其他cgroup中时，它所占用的内存是否也随着移动过去 memory.oom_control #设置/显示oom controls相关的配置 memory.numa_stat #显示numa相关的内存  #### cpuacct\ncpuacct.usage #所有cpu核的累加使用时间(nanoseconds) cpuacct.usage_percpu #针对多核，输出的是每个CPU的使用时间(nanoseconds) cpuacct.stat #输出系统（system/kernel mode）耗时和用户（user mode）耗时 ， 单位为USER_HZ。  Storage Driver aufs(UnionFS)  容器启动速度很快\n 存储空间利用很高效\n 内存的利用很高效\n  读写：写时复制\n删除：whiteout 屏蔽\nDocker 镜像的各层的全部内容都存储在/var/lib/docker/aufs/diff/\u0026lt;image-id\u0026gt;文件夹下，每个文件夹下包含了该镜像层的全部文件和目录，文件以各层的 UUID 命名。\n正在运行的容器的文件系统被挂载在/var/lib/docker/aufs/mnt/\u0026lt;container-id\u0026gt;文件夹下，这就是 AUFS 的联合挂载点，在这里的文件夹下，你可以看到容器文件系统的所有文件。如果容器没有在运行，它的挂载目录仍然存在，不过是个空文件夹。\n容器的元数据和各种配置文件被放在/var/lib/docker/containers/\u0026lt;container-id\u0026gt;文件夹下，无论容器是运行还是停止都会有一个文件夹。如果容器正在运行，其对应的文件夹下会有一个 log 文件。\n容器的只读层存储在/var/lib/docker/aufs/diff/\u0026lt;container-id\u0026gt;目录下，对容器的所有修改都会保存在这个文件夹下，即便容器停止，这个文件夹也不会删除。也就是说，容器重启后并不会丢失原先的更改。\n容器中镜像层的信息存储在/var/lib/docker/aufs/layers/\u0026lt;container-id\u0026gt;文件中。文件中从上至下依次记录了容器使用的各镜像层。\n性能表现  在容器密度比较告的场景下，AUFS 是非常好的选择，因为AUFS的容器间共享镜像层的特性使其磁盘利用率很高，容器的启动时间很短\n AUFS 中容器之间的共享使对系统页缓存的利用率很高\n AUFS 的写时复制策略会带来很高的性能开销，因为 AUFS 对文件的第一次更改需要将整个文件复制带读写层，当容器层数很多或文件所在目录很深时尤其明显\n  device mapper device mapper工作在块层次上而不是文件层次上，这意味着它的写时复制策略不需要拷贝整个文件。\n在device mapper中，对容器的写操作由需要时分配策略完成。更新已有数据由写时复制策略完成，这些操作都在块的层次上完成，每个块的大小为64KB。\n需要时分配(allocate-on-demand) 每当容器中的进程需要向容器写入数据时，device mapper就从资源池中分配一些数据块并将其映射到容器。当容器频繁进行小数据的写操作时，这种机制非常影响影响性能。\n写时复制(copy-on-write) device mapper的写时复制策略以64KB作为粒度，意味着无论是对32KB的文件还是对1GB大小的文件的修改都仅复制64KB大小的文件。这相对于在文件层面进行的读操作具有很明显的性能优势。但是，如果容器频繁对小于64KB的文件进行改写，device mapper的性能是低于aufs的。\noverlayfs(UnionFS) OverlayFS与AUFS相似，也是一种联合文件系统(union filesystem)，与AUFS相比，OverlayFS：\n- 设计更简单\n- 被加入Linux3.18版本内核\n- 可能更快\nOverlayFS 将一个 Linux 主机中的两个目录组合起来，一个在上，一个在下，对外提供统一的视图。这两个目录就是层layer，将两个层组合在一起的技术被成为联合挂载union mount。在OverlayFS中，上层的目录被称作upperdir，下层的目录被称作lowerdir，对外提供的统一视图被称作merged。\nOverlayFS 仅有两层，也就是说镜像中的每一层并不对应 OverlayFS 中的层，而是镜像中的每一层对应/var/lib/docker/overlay中的一个文件夹，文件夹以该层的 UUID 命名。然后使用硬连接将下面层的文件引用到上层。这在一定程度上节省了磁盘空间。这样 OverlayFS中 的lowerdir就对应镜像层的最上层，并且是只读的。在创建镜像时，Docker 会新建一个文件夹作为OverlayFS的upperdir，它是可写的。\n读写：第一次修改时，文件不在container layer(upperdir)中，overlay driver 调用copy-up操作将文件从lowerdir读到upperdir中，然后对文件的副本做出修改。\noverlay的copy-up操作工作在文件层面, 对文件的修改需要将整个文件拷贝到upperdir中。\n- copy-up操作仅发生在文件第一次被修改时，此后对文件的读写都直接在upperdir中进行\n- overlayfs中仅有两层，这使得文件的查找效率很高(相对于aufs)。\n删除：whiteout 覆盖\n参考 https://yq.aliyun.com/articles/54483\nhttps://segmentfault.com/a/1190000008323952\nhttps://blog.csdn.net/vchy_zhao/article/details/70238690\n"
},
{
	"uri": "/notes/publish-your-site-on-k8s/",
	"title": "将网站部署在 Kubernetes 上",
	"tags": [],
	"description": "",
	"content": " Kubernetes 环境 之前在 阿里云主机搭建-k8s-集群 这篇文章介绍了如何在阿里云环境快速搭建一个 Kubernetes 环境，按照文章的步骤，可以快速搭建一个可用的 Kubernetes 集群。\n网站 同样在 Build Blog With Hugo 这边文章中，介绍了怎么使用 hugo 快速搭建一个自己的个人博客。\n容器化 我的项目 hugo-dcos 包含了容器化一个 hugo 网站项目所需的一些脚本和 Dockerfile, 可以参考本项目自行容器化自己的项目。\n项目使用了webhook接收 github 的通知，并在github上配置项目maoqide.github.io的 Webhooks, 当本项目有更新时，会调用 webhook 服务触发操作，拉取最新的代码。\n部署 本文介绍如何将自己的个人博客通过 Kubernetes 发布到公网，让大家可以访问。\n\u0026gt; 当然，最简单的发布方法是通过 Github Pages, 直接将 hugo 生成的 publish 文件夹上传到自己 github 的命名为 your-username.github.io 的仓库下，即可以通过 https://your-username.github.io 访问到自己的网站。\n准备  域名 云服务器  创建 Deployment \u0026amp; Service 首先创建 Deployment 和 Service。\napiVersion: apps/v1 kind: Deployment metadata: name: mysite spec: selector: matchLabels: app: site replicas: 1 template: metadata: labels: app: site spec: containers: - name: mysite image: maoqide/site:v1.1 env: - name: GITHUB_HOOK_SECRET value: MY_SECRET ports: - containerPort: 80 - containerPort: 9000 livenessProbe: httpGet: # scheme: HTTPS path: / port: 80 initialDelaySeconds: 15 timeoutSeconds: 1 --- kind: Service apiVersion: v1 metadata: name: mysite spec: selector: app: site ports: - name: nginx protocol: TCP port: 80 targetPort: 80 - name: webhook protocol: TCP port: 9000 targetPort: 9000  创建 ingress-controller 为了能够从外部访问，还需要创建 Ingress。\n这里使用nginx ingress，首先要创建 nginx-ingress-controller。\nkind: ConfigMap apiVersion: v1 metadata: name: nginx-configuration labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx --- kind: ConfigMap apiVersion: v1 metadata: name: tcp-services labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx --- kind: ConfigMap apiVersion: v1 metadata: name: udp-services labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx --- apiVersion: v1 kind: ServiceAccount metadata: name: nginx-ingress-serviceaccount labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRole metadata: name: nginx-ingress-clusterrole labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx rules: - apiGroups: - \u0026quot;\u0026quot; resources: - configmaps - endpoints - nodes - pods - secrets verbs: - list - watch - apiGroups: - \u0026quot;\u0026quot; resources: - nodes verbs: - get - apiGroups: - \u0026quot;\u0026quot; resources: - services verbs: - get - list - watch - apiGroups: - \u0026quot;extensions\u0026quot; resources: - ingresses verbs: - get - list - watch - apiGroups: - \u0026quot;\u0026quot; resources: - events verbs: - create - patch - apiGroups: - \u0026quot;extensions\u0026quot; resources: - ingresses/status verbs: - update --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: Role metadata: name: nginx-ingress-role labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx rules: - apiGroups: - \u0026quot;\u0026quot; resources: - configmaps - pods - secrets - namespaces verbs: - get - apiGroups: - \u0026quot;\u0026quot; resources: - configmaps resourceNames: # Defaults to \u0026quot;\u0026lt;election-id\u0026gt;-\u0026lt;ingress-class\u0026gt;\u0026quot; # Here: \u0026quot;\u0026lt;ingress-controller-leader\u0026gt;-\u0026lt;nginx\u0026gt;\u0026quot; # This has to be adapted if you change either parameter # when launching the nginx-ingress-controller. - \u0026quot;ingress-controller-leader-nginx\u0026quot; verbs: - get - update - apiGroups: - \u0026quot;\u0026quot; resources: - configmaps verbs: - create - apiGroups: - \u0026quot;\u0026quot; resources: - endpoints verbs: - get --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: RoleBinding metadata: name: nginx-ingress-role-nisa-binding labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: nginx-ingress-role subjects: - kind: ServiceAccount name: nginx-ingress-serviceaccount --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: nginx-ingress-clusterrole-nisa-binding labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: nginx-ingress-clusterrole subjects: - kind: ServiceAccount name: nginx-ingress-serviceaccount namespace: default --- apiVersion: extensions/v1beta1 kind: Deployment metadata: name: nginx-ingress-controller labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx spec: replicas: 1 selector: matchLabels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx template: metadata: labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx annotations: prometheus.io/port: \u0026quot;10254\u0026quot; prometheus.io/scrape: \u0026quot;true\u0026quot; spec: serviceAccountName: nginx-ingress-serviceaccount containers: - name: nginx-ingress-controller image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.21.0 args: - /nginx-ingress-controller - --configmap=$(POD_NAMESPACE)/nginx-configuration - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services - --udp-services-configmap=$(POD_NAMESPACE)/udp-services - --publish-service=$(POD_NAMESPACE)/ingress-nginx - --annotations-prefix=nginx.ingress.kubernetes.io securityContext: capabilities: drop: - ALL add: - NET_BIND_SERVICE # www-data -\u0026gt; 33 runAsUser: 33 env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace ports: - name: http containerPort: 80 - name: https containerPort: 443 livenessProbe: failureThreshold: 3 httpGet: path: /healthz port: 10254 scheme: HTTP initialDelaySeconds: 10 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 1 readinessProbe: failureThreshold: 3 httpGet: path: /healthz port: 10254 scheme: HTTP periodSeconds: 10 successThreshold: 1 timeoutSeconds: 1 ---  以上 yaml 创建了一个 nginx-ingress-controller 的 Deployment，并赋给 Deployment 下的 Pod 对 Ingress 等集群内资源的 API 访问权限。\n接着要为 nginx-ingress-controller 创建 Node port 类型的 Service，让我们可以通过云服务器的公网地址访问到 ingress:\napiVersion: v1 kind: Service metadata: name: ingress-nginx labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx spec: type: NodePort ports: - name: http port: 80 targetPort: 80 protocol: TCP - name: https port: 443 targetPort: 443 protocol: TCP selector: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx ---  创建 Ingress 创建 Ingress 策略。\napiVersion: extensions/v1beta1 kind: Ingress metadata: name: site-ingress annotations: nginx.ingress.kubernetes.io/rewrite-target: / nginx.ingress.kubernetes.io/ssl-redirect: \u0026quot;false\u0026quot; spec: rules: - http: paths: - path: /_hook backend: serviceName: mysite servicePort: 9000 - path: / backend: serviceName: mysite servicePort: 80  配置域名 上述步骤完成，已经可以通过云服务器公网IP= https://IP:ingress_svc-port 访问网站了。\n为了访问方便，还可以给站点配置一个域名，直接在阿里云购买一个域名，然后通过绑定一个 A 类型的域名解析，解析到阿里云公网IP，就可以通过域名加端口访问了。\n如果不想访问的时候加上端口，还需要再配置一个 隐性URL 的域名解析（需要先将域名备案）。\n附件 以下是上面用到的 yaml 文件：\n- site-deployment\n- nginx-ingress-controller\n- ingress-svc\n- ingress\n"
},
{
	"uri": "/golang/",
	"title": "Golang",
	"tags": [],
	"description": "",
	"content": "documents about golang.\n"
},
{
	"uri": "/algorithm/basic-data-structure/",
	"title": "Basic Data Structure",
	"tags": [],
	"description": "",
	"content": " Array 数组 是线性表数据结构。用一组连续存储空间，储存一组相同类型的数据。支持随机访问 O(1)。\n低效插入和删除 O(n)，需要数据搬移。优化：删除操作延后(JVM垃圾回收算法)。\nLinked list 链表 插入删除 O(1)，随机访问O(n)。\n单链表：插入删除时需要查找前驱节点 O(n)。\n双向链表：插入删除时查找前驱结点 O(1)。\n链表代码实现 单链表字符串回文判断\n 单链表反转\n 链表中环的检测\n 两个有序的链表合并 删除链表倒数第 n 个结点 求链表中间节点  Stack 栈 后进先出，先进后出。入栈出栈 O(1)\n顺序栈：数组。动态扩容（动态扩容数组，数据搬移O(n)）\n链式栈：链表。\nArrayStack 实现 LinkedListStack 实现\n  Queue 队列 先进先出，后进后出。 入队出队O(1)\n数组实现(数据搬移) 链表实现 循环链表数组实现  Skip list 跳表 链表加多级索引的数据结构。查找 O(logn)。Redis 中有序集合 Sorted Set 用跳表实现。\n插入删除 O(logn)。 平衡性维护-随机函数。\n实现   Hash Table 散列表 通过散列函数把元素的键值映射为下标，然后将数据存储在数组中对应下标的位置。支持随机访问 O(1)。\n散列函数。\n装载因子 = 填入表中的元素个数 / 散列表的长度\n解决散列冲突：\n- 开放寻址法\n优点：可以有效利用CPU缓存加快查询速度；便于序列化。\n缺点：删除时不能直接删除，需要特使标记已删除的数据；冲突代价高。\n- 线性探测(ThreadLocalMap)：当往散列表中插入数据时，如果某个数据经过散列函数散列之后，存储位置已经被占用了，就从当前位置开始，依次往后查找，看是否有空闲位置，直到找到为止。(hash(key)+n)\n- 二次探测：hash(key)+n^2\n- 双重散列：使用多组散列函数(hash1(key)，hash2(key)，hash3(key))\n- 链表法(LinkedHashMap)\n优点：内存利用率高，链表节点不需要提前申请；大装载因子容忍度高。\n缺点：CPU缓存不友好；由于要存储指针，存储小对象时内存消耗大。\n动态扩容：避免一次性搬移大量数据。扩容时只申请新空间，新的数据插入新的散列表，同时将小部分老数据搬移至新散列表，将扩容消耗均摊至每次插入。\n散列表+双向链表 查找删除添加 O(1):\n\u0026gt; LRU 缓存淘汰算法/Redis 有序集合/Java LinkedHashMap\n散列表是通过链表法解决散列冲突的，所以每个结点会在两条链中。一个链是刚刚我们提到的双向链表，另一个链是散列表中的拉链。前驱和后继指针是为了将结点串在双向链表中，hnext 指针是为了将结点串在散列表的拉链中。\n哈希算法：\n将任意长度的二进制值串映射为固定长度的二进制值串的算法。\n应用：\n1. 安全加密(MD5/SHA/AES)\n2. 唯一标识\n3. 数据校验\n4. 散列函数\n5. 负载均衡(会话粘滞, 对IP或会话ID哈希)\n6. 数据分片(大文件关键词分片)\n7. 分布式存储(机器扩容导致重新计算哈希值，一致性哈希算法)\npractice-code\n"
},
{
	"uri": "/notes/build-blog-with-hugo/",
	"title": "Build Blog With Hugo",
	"tags": [],
	"description": "",
	"content": " documentations https://gohugo.io/getting-started/usage/ https://learn.netlify.com/en/\nsetup  install hugo following offical install guide.\ndownload packages from Hugo Release and put executale file hugo in PATH.\n execute hugo new site sitename, To create a new site. directory structure will like this:\n. ├── archetypes ├── assets ├── config.toml ├── content ├── data ├── layouts ├── static └── themes  download theme from github. unzip the archive and copy files to you site dir, overwriting directory and files of the same name. you can change your website config by changing config.toml.\n wirte you first blog. hugo new content/first.md, will gen a markdown file in content. you can use hugo server -w to start a server locally, and visit localhost:1313 for preview.\n execute hugo, this will gen md files in dir content to html files in dir public.\n put your public directory in a nginx.\n  menu directorys in directory content will display as menu in the left. and file named _index.md is the default page of the directory.\nexample you can visit my github repo hugo-blog, the article itself is generated by this repo.\nchange theme you can change your styles by changing different themes. themes could be found at http://themes.gohugo.io.\noverall overall, you can easily create your own doc/blog by follow command:\n# create a new site hugo new site mysite cd mysite/ # create your first page hugo new first.md # build page hugo  then you can find your site in directory public.\n"
},
{
	"uri": "/collection/",
	"title": "Collection",
	"tags": [],
	"description": "",
	"content": "Collection excellent articles.\n"
},
{
	"uri": "/collection/collection-1/",
	"title": "Collection 1",
	"tags": [],
	"description": "",
	"content": " Mesos 的资源分配，解释 mesos 的二级调度和资源邀约机制，并详细介绍了保证资源分配公平的DRF算法（主导资源公平算法 Dominant Resource Fairness）。\nhttps://www.infoq.cn/article/analyse-mesos-part-04\n备用链接\n Kubernetes调度详解。\nhttp://dockone.io/article/2885\n备用链接\nhttps://cizixs.com/2017/07/19/kubernetes-scheduler-source-code-analysis/\n备用链接\n Istio 调用链埋点/tracing: 使用 istio 做调用链跟踪依旧需要修改一定业务代码，微服务间请求时需要配合在请求头上传递 sidecar 生成的 Trace 相关信息，才能将调用关联到同一个 trace 上，否则每次请求都会形成新的割裂的 Trace 信息。\nhttps://www.infoq.cn/article/pqy*PFPhox9OQQ9iCRTt\n备用链接\n kubernetes informer 解析。\nhttps://blog.csdn.net/weixin_42663840/article/details/81699303\n  "
},
{
	"uri": "/algorithm/",
	"title": "Algorithm",
	"tags": [],
	"description": "",
	"content": " documents about algorithm.\n "
},
{
	"uri": "/cloud/",
	"title": "Cloud",
	"tags": [],
	"description": "",
	"content": " documents about cloud.\nkubernetes/docker\u0026hellip;\n "
},
{
	"uri": "/cloud/controller-manager-%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/",
	"title": "Controller Manager 源码阅读",
	"tags": [],
	"description": "",
	"content": "k8s controller-manager 源码阅读笔记\n代码结构 本部分用于记录 kube-controller-manager 代码整体结构及关键方法，便于到源码中查找，个人阅读记录，读者可跳过。本文所有代码均基于 kubernetes release-1.13。  // cmd/kube-controller-manager/controller-manager.go main() command := app.NewControllerManagerCommand() // cmd/kube-controller-manager/app/controllermanager.go NewControllerManagerCommand() /* **\t指定 port，所有默认 port 在 pkg/master/ports/ports.go 这个文件中 **\t将默认配置传入 Options，设置每个 controller 基础配置（证书，同步间隔...） **\t设置 gcIgnoredResources **\tvar ignoredResources = map[schema.GroupResource]struct{}{ **\t{Group: \u0026quot;\u0026quot;, Resource: \u0026quot;events\u0026quot;}: {}, **\t} */ NewKubeControllerManagerOptions()\t// creates a new KubeControllerManagerOptions with a default config. cmd := \u0026amp;cobra.Command{ // ... Run: func (cmd *cobra.Command, args []string) {} //... s.Config(KnownControllers(), ControllersDisabledByDefault.List()) // ControllersDisabledByDefault = sets.NewString(\u0026quot;bootstrapsigner\u0026quot;, \u0026quot;tokencleaner\u0026quot;,) s.Validate(allControllers, disabledByDefaultControllers)\t// validate all controllers options and config client, err := clientset.NewForConfig(restclient.AddUserAgent(kubeconfig, KubeControllerManagerUserAgent)) leaderElectionClient := clientset.NewForConfigOrDie(restclient.AddUserAgent(\u0026amp;config, \u0026quot;leader-election\u0026quot;)) // an EventRecorder can be used to send events to this EventBroadcaster // with the event source set to the given event source. eventRecorder := createRecorder(client, KubeControllerManagerUserAgent) func createRecorder(kubeClient clientset.Interface, userAgent string) record.EventRecorder {} eventBroadcaster.StartRecordingToSink(\u0026amp;v1core.EventSinkImpl{Interface: kubeClient.CoreV1().Events(\u0026quot;\u0026quot;)}) return eventBroadcaster.NewRecorder(clientgokubescheme.Scheme, v1.EventSource{Component: userAgent}) c := \u0026amp;kubecontrollerconfig.Config{ Client: client, Kubeconfig: kubeconfig, EventRecorder: eventRecorder, LeaderElectionClient: leaderElectionClient, }\tKnownControllers() // NewControllerInitializers is a public map of named controller groups (you can start more than one in an init func) // paired to their InitFunc. NewControllerInitializers(loopMode ControllerLoopMode) map[string]InitFunc {} controllers[\u0026quot;endpoint\u0026quot;] = startEndpointController controllers[\u0026quot;replicationcontroller\u0026quot;] = startReplicationController controllers[\u0026quot;podgc\u0026quot;] = startPodGCController controllers[\u0026quot;resourcequota\u0026quot;] = startResourceQuotaController controllers[\u0026quot;namespace\u0026quot;] = startNamespaceController controllers[\u0026quot;serviceaccount\u0026quot;] = startServiceAccountController controllers[\u0026quot;garbagecollector\u0026quot;] = startGarbageCollectorController controllers[\u0026quot;daemonset\u0026quot;] = startDaemonSetController controllers[\u0026quot;job\u0026quot;] = startJobController controllers[\u0026quot;deployment\u0026quot;] = startDeploymentController controllers[\u0026quot;replicaset\u0026quot;] = startReplicaSetController controllers[\u0026quot;horizontalpodautoscaling\u0026quot;] = startHPAController controllers[\u0026quot;disruption\u0026quot;] = startDisruptionController controllers[\u0026quot;statefulset\u0026quot;] = startStatefulSetController controllers[\u0026quot;cronjob\u0026quot;] = startCronJobController controllers[\u0026quot;csrsigning\u0026quot;] = startCSRSigningController controllers[\u0026quot;csrapproving\u0026quot;] = startCSRApprovingController controllers[\u0026quot;csrcleaner\u0026quot;] = startCSRCleanerController controllers[\u0026quot;ttl\u0026quot;] = startTTLController controllers[\u0026quot;bootstrapsigner\u0026quot;] = startBootstrapSignerController controllers[\u0026quot;tokencleaner\u0026quot;] = startTokenCleanerController controllers[\u0026quot;nodeipam\u0026quot;] = startNodeIpamController controllers[\u0026quot;nodelifecycle\u0026quot;] = startNodeLifecycleController controllers[\u0026quot;persistentvolume-binder\u0026quot;] = startPersistentVolumeBinderController controllers[\u0026quot;attachdetach\u0026quot;] = startAttachDetachController controllers[\u0026quot;persistentvolume-expander\u0026quot;] = startVolumeExpandController controllers[\u0026quot;clusterrole-aggregation\u0026quot;] = startClusterRoleAggregrationController controllers[\u0026quot;pvc-protection\u0026quot;] = startPVCProtectionController controllers[\u0026quot;pv-protection\u0026quot;] = startPVProtectionController controllers[\u0026quot;ttl-after-finished\u0026quot;] = startTTLAfterFinishedController controllers[\u0026quot;root-ca-cert-publisher\u0026quot;] = startRootCACertPublisher Run(c.Complete(), wait.NeverStop) configz.New(ConfigzName)\t// ConfigzName=\u0026quot;kubecontrollermanager.config.k8s.io\u0026quot; // Start the controller manager HTTP server. insecure as example. unsecuredMux = genericcontrollermanager.NewBaseHandler(\u0026amp;c.ComponentConfig.Generic.Debugging, checks...) insecureSuperuserAuthn := server.AuthenticationInfo{Authenticator: \u0026amp;server.InsecureSuperuser{}} handler := genericcontrollermanager.BuildHandlerChain(unsecuredMux, nil, \u0026amp;insecureSuperuserAuthn) InsecureServing.Serve(handler, 0, stopCh) RunServer(insecureServer, s.Listener, shutdownTimeout, stopCh) run := func(ctx context.Context) {} rootClientBuilder := controller.SimpleControllerClientBuilder{ ClientConfig: c.Kubeconfig, } controllerContext, err := CreateControllerContext(c, rootClientBuilder, clientBuilder, ctx.Done()) // CreateControllerContext creates a context struct containing references to resources needed by the // controllers such as the cloud provider and clientBuilder. rootClientBuilder is only used for // the shared-informers client and token controller. func CreateControllerContext(s *config.CompletedConfig, rootClientBuilder, clientBuilder controller.ControllerClientBuilder, stop \u0026lt;-chan struct{}) (ControllerContext, error) {} versionedClient := rootClientBuilder.ClientOrDie(\u0026quot;shared-informers\u0026quot;) sharedInformers := informers.NewSharedInformerFactory(versionedClient, ResyncPeriod(s)()) // If apiserver is not running we should wait for some time and fail only then. This is particularly // important when we start apiserver and controller manager at the same time. genericcontrollermanager.WaitForAPIServer(versionedClient, 10*time.Second) // returns the supported resources for all groups and versions, by request \u0026quot;/apis/.......\u0026quot; availableResources, err := GetAvailableResources(rootClientBuilder) ctx := ControllerContext{ ClientBuilder: clientBuilder, InformerFactory: sharedInformers, ComponentConfig: s.ComponentConfig, RESTMapper: restMapper, AvailableResources: availableResources, Cloud: cloud, LoopMode: loopMode, Stop: stop, InformersStarted: make(chan struct{}), ResyncPeriod: ResyncPeriod(s), } // serviceAccountTokenControllerStarter is special because it must run first to set up permissions for other controllers. // It cannot use the \u0026quot;normal\u0026quot; client builder, so it tracks its own. It must also avoid being included in the \u0026quot;normal\u0026quot; // init map so that it can always run first. saTokenControllerInitFunc := serviceAccountTokenControllerStarter{rootClientBuilder: rootClientBuilder}.startServiceAccountTokenController startServiceAccountTokenController() {} // 获取相关证书 // ... controller, err := serviceaccountcontroller.NewTokensController( ctx.InformerFactory.Core().V1().ServiceAccounts(), ctx.InformerFactory.Core().V1().Secrets(), c.rootClientBuilder.ClientOrDie(\u0026quot;tokens-controller\u0026quot;), serviceaccountcontroller.TokensControllerOptions{ TokenGenerator: tokenGenerator, RootCA: rootCA, }, ) NewTokensController() {}\t// watch queueServiceAccountSync event 和 queueSecretSync event StartControllers(controllerContext, saTokenControllerInitFunc, NewControllerInitializers(controllerContext.LoopMode), unsecuredMux) startSATokenController(ctx)\t// 先启动 SATokenController for controllerName, initFn := range controllers { IsControllerEnabled\t// 判断是否启用 controller // Jitter returns a time.Duration between duration and duration + maxFactor * duration. // This allows clients to avoid converging on periodic behavior. If maxFactor // is 0.0, a suggested default value will be chosen. time.Sleep(wait.Jitter(ctx.ComponentConfig.Generic.ControllerStartInterval.Duration, ControllerStartJitter)) debugHandler, started, err := initFn(ctx) // if debugHandler != nil \u0026amp;\u0026amp; unsecuredMux != nil { basePath := \u0026quot;/debug/controllers/\u0026quot; + controllerName unsecuredMux.UnlistedHandle(basePath, http.StripPrefix(basePath, debugHandler)) unsecuredMux.UnlistedHandlePrefix(basePath+\u0026quot;/\u0026quot;, http.StripPrefix(basePath, debugHandler)) } } controllerContext.InformerFactory.Start(controllerContext.Stop) // InformersStarted is closed after all of the controllers have been initialized and are running. After this point it is safe, // for an individual controller to start the shared informers. Before it is closed, they should not. close(controllerContext.InformersStarted)\t// !!!!! // leader 选举，通过获取锁成为 leader leaderelection.RunOrDie(context.TODO(), leaderelection.LeaderElectionConfig{ Lock: rl, // ... ... Callbacks: leaderelection.LeaderCallbacks{ OnStartedLeading: run, OnStoppedLeading: func() { klog.Fatalf(\u0026quot;leaderelection lost\u0026quot;) }, }, WatchDog: electionChecker, Name: \u0026quot;kube-controller-manager\u0026quot;, }) }  cmd.Run 目前的 kubernetes 组件全都采用 cobra 构建。核心的启动流程都在生成的 cobra.Command 实例 cmd 的 Run() 方法中。\nRun 方法执行了两个方法，s.Config(KnownControllers(), ControllersDisabledByDefault.List()), Run(c.Complete(), wait.NeverStop)。\ns.Config(KnownControllers(), ControllersDisabledByDefault.List()) KnownControllers() 方法调用了 NewControllerInitializers(), 生成一个 map[string]InitFunc{} 的map，保存了 controller 和对应的启动方法的映射。s.Config 方法返回了 controller-manager 的配置，首先对所有 controllers 进行 validate，然后会构建一个 k8s clientset 和 一个 EventRecorder 对象，具体事件记录的方式，会在后面分析。\n// EventRecorder knows how to record events on behalf of an EventSource. // an EventRecorder can be used to send events to this EventBroadcaster // with the event source set to the given event source. type EventRecorder interface { // Event constructs an event from the given information and puts it in the queue for sending. // The resulting event will be created in the same namespace as the reference object. Event(object runtime.Object, eventtype, reason, message string) // Eventf is just like Event, but with Sprintf for the message field. Eventf(object runtime.Object, eventtype, reason, messageFmt string, args ...interface{}) // PastEventf is just like Eventf, but with an option to specify the event's 'timestamp' field. PastEventf(object runtime.Object, timestamp metav1.Time, eventtype, reason, messageFmt string, args ...interface{}) // AnnotatedEventf is just like eventf, but with annotations attached AnnotatedEventf(object runtime.Object, annotations map[string]string, eventtype, reason, messageFmt string, args ...interface{}) }  Run(c.Complete(), wait.NeverStop) 首先configz.New(ConfigzName)生成 controller-manager 的配置对象，用名称kubecontrollermanager.config.k8s.io注册到注册到路由/configz上，可以直接通过 controller-manager 的地址访问到。接着，会新建一个 BaseHandler 实例并启动 HTTP server，用来注册 controller manager 的 /metric 和 /healthz 接口。接下来，定义了一个内部方法run(), 启动 controller-manager 的主要操作，都在这个方法中完成。\nrun()方法首先创建一个ControllerContext, 结构体如下，主要包含 controllers 所需要资源的引用，如 kubernetes 的 clientset 和informer。AvailableResources 由GetAvailableResources方法获取，通过 apiserver 的/api/...接口获取集群支持的所有 group 和 version 资源。然后开始启动 controllers，在启动其他 controller 前，一定先启动 SATokenController，因为它必须先为其他 controller 创建所需的 token 授权，所以启动SATokenController的方法独立于启动其他 controller 的循环之外，作为最先启动的一个。startServiceAccountTokenController watch 了ServiceAccount和secret的增删改事件，并同步本地的 queue 缓存。接下来才会调用StartControllers方法依次调用其他 controller 的 InitFunc 启动 controllers。最后启动 controllerContext 的 InformerFactory 并执行close(controllerContext.InformersStarted)。这里的controllerContext.InformersStarted是chan struct{}类型，当这个 channel 被关闭了，代表所有 controllers 都被初始化并运行起来了，独立的 controller 应该在他关闭后再启动 sharedInformer。\ntype ControllerContext struct { // ClientBuilder will provide a client for this controller to use ClientBuilder controller.ControllerClientBuilder // InformerFactory gives access to informers for the controller. InformerFactory informers.SharedInformerFactory // ComponentConfig provides access to init options for a given controller ComponentConfig kubectrlmgrconfig.KubeControllerManagerConfiguration // DeferredDiscoveryRESTMapper is a RESTMapper that will defer // initialization of the RESTMapper until the first mapping is // requested. RESTMapper *restmapper.DeferredDiscoveryRESTMapper // AvailableResources is a map listing currently available resources AvailableResources map[schema.GroupVersionResource]bool // Cloud is the cloud provider interface for the controllers to use. // It must be initialized and ready to use. Cloud cloudprovider.Interface // Control for which control loops to be run // IncludeCloudLoops is for a kube-controller-manager running all loops // ExternalLoops is for a kube-controller-manager running with a cloud-controller-manager LoopMode ControllerLoopMode // Stop is the stop channel Stop \u0026lt;-chan struct{} // InformersStarted is closed after all of the controllers have been initialized and are running. After this point it is safe, // for an individual controller to start the shared informers. Before it is closed, they should not. InformersStarted chan struct{} // ResyncPeriod generates a duration each time it is invoked; this is so that // multiple controllers don't get into lock-step and all hammer the apiserver // with list requests simultaneously. ResyncPeriod func() time.Duration }  最后，会执行leaderelection.RunOrDie方法不断尝试获取或更新(tryAcquireOrRenew) leader lease，成功获取到的即为 leader，并执行上面的run()方法。实际上就是去获取一个锁并有一定有效期，当一个 contoller-manager 实例获取到锁并且超出了有效期，那么这个实例就会成为leader，成为 leader 的实例仍然会不断执行tryAcquireOrRenew来更新获取时间AcquireTime以延长有效期。(leader 选举的逻辑在 k8s.io\\client-go\\tools\\leaderelection\\leaderelection.go)\ncontrollers 上面就是 controller-manager 的整体启动流程和代码结构。接下来会具体分析几个核心的 controller。\nreplicasetcontroller kubernetes\\pkg\\controller\\replicaset\\replica_set.go\n"
},
{
	"uri": "/golang/goroutine/",
	"title": "Goroutine 的管理",
	"tags": [],
	"description": "",
	"content": " goroutine 是 go 的最重要特性之一，可以方便的实现并发编程。但是真正用起来，如果不多加注意，很容易造成 goroutine 的泄漏或者脱离管理，造成代码跑一段时间，就是产生大量无法回收的goroutine(可通过 pprof 查看)。最近学习整理了下 go 语言中管理 goroutine 的几种方法和一些最佳实践。\n几点原则 go-best-practices-concurrency\n在 github上的 go-best-practices 项目中，提到了几点最佳实践，这里记录下。\n不要和 goroutine 失去联系  Don\u0026rsquo;t loose contact with your goroutines\n 如何避免? 使用make(chan struct{})/sync.WaitGroup/context.Context或select。\n你可能需要这样：\n1. 当必要的时候可以*中断*创建的 goroutine。\n2. 等待直到产生的所有 goroutine 都完成了。\n中断(Interruption)\n可以用以下方式实现：\n1. 共享一个无缓冲的空结构体通道（make（chan struct {}）），由 goroutine 的创建者发出关闭信号以关闭。\n2. 一个可取消的context.Context。\n3. 确保你的 goroutine 使用select来不时检查他们的信号，而不会无限期地阻塞住。\n等待 goroutine 完成(Waiting for goroutines to finish)\n实现的最简单方法是使用sync.WaitGroup。在创建 goroutine 之前，请确保调用了wg.Add(1)。在运行 goroutine 之后，但在它 return 之前，请确保wg.Done()。这种场景下，defer是很好的选择。\n不要用 WaitGroup 来计数多种类型的 goroutine  Don\u0026rsquo;t use wait groups to count more than one type of goroutine\n 这里说的 gouroutine 的类型和被作为 gouroutine 调用的函数相关联，此函数可以是另一种类型的成员函数，可以是包中的命名函数，也可以是匿名函数。重要的一点是，你不应该在作为goroutine 调用的不同函数之间共享 WaitGroup。保持简单，如果你需要对一个不同类型的函数使用go关键字，创建一个新的 WaitGroup，并对它正确命名。\ntype Parent struct { wgFoo sync.WaitGroup wgBar sync.WaitGroup } func (p *Parent) foo() { defer p.wgFoo.Done() } func (p *Parent) bar() { defer p.wgBar.Done() } func (p *Parent) Go() { p.wgFoo.Add(1) go p.foo() p.wgBar.Add(1) go.bar() }  虽然共享一个 WaitGroup 可能是正确的解决方案，但是当下一位工程师接受时，它会增加问题的认知复杂性。\n不要让一个 channel 的消费者说什么时候结束  Don\u0026rsquo;t let a channel consumer say when it is done\n 对一个已关闭的 channel 发送会导致 panic\n首先且最重要的是，代码是基于 channel 的消费者和生产者模型的实现，这本身就是一种很好的做法。这是一个明显的关注点分离。\ngolang 给你在编译时定义一个 channel 的方向的能力recvOnly \u0026lt;-chan Thing := make(chan Thing)。这在定义变量时很少有用，但是，在定义函数的接收参数时非常有用。比如：\nfunc consume(things \u0026lt;-chan Thing) { // will do work until close for thing := range things { // do work } }  这强制（在编译时）消费者 goroutine 无法在对 channel 发送数据，包括关闭该 channel 的能力。\n这强制顶一个租户(goroutine)安全管理 channel。只有当所有生产者停止发送，才关闭 channel。谨记对一个已关闭的 channel 发送会导致 panic。\n关闭 channel 的代码必须选保证不会再对此 channel 发送\n\u0026gt; The piece of code which closes a channel must first guarantee that nothing else will produce on it\n如果所有对 channel 的发送都在关闭前同步发生，只要你不重试并再次发送，那就是安全的。\n如果该 channel 上的生产(production)被放弃到其他 goroutine，那么你需要能够与这些 goroutine 同步退出。\n如果我们可以保证对 goroutine 进行计数并等待它们退出，那么我们可以确定关闭 channel 不会在其他地方引起 panic。\nfunc doConcurrently() { var ( things = make(chan Thing) finished = make(chan struct{}) wg sync.WaitGroup ) go func() { // will consume until close consume(things) // signal consumption has finished close(finished) }() for i := 0; i \u0026lt; noOfThingsWeWantToDo; i++ { wg.Add(1) go func() { defer wg.Done() things \u0026lt;- Thing{} }() } // wait until all producers have stopped wg.Wait() // then you can close close(things) // wait until finished consuming \u0026lt;-finished }  总结  确保消费者只能消费。使用recvOnly \u0026lt;-chan Thing 。\n 跟踪 gouroutine 的完成。使用sync.WaitGroup。\n 只有在确认生产者 goroutine 不能再对 channel 进行发送的情况下，再关闭channel。\n  从外部结束一个 goroutine [参考]从外部结束一个 goroutine\n可响应 channel 的 goroutine\n最直接的方法是关闭与这个 goroutine 通信的 channel close(ch)。如果这个 goroutine 此时阻塞在 read 上，那么阻塞会失效，并在第二个返回值中返回 false (此时可以检测并退出)；如果阻塞在 write 上，那么会 panic，这时合理的做法是在 goroutine 的顶层 recover 并退出。 更健壮的设计一般会把 data channel (用于传递业务逻辑的数据) 和 signal channel (用于管理 goroutine 的状态) 分开。不会让 goroutine 直接读写 data channel，而是通过 select-default 或 select-timeout 来避免完全阻塞，同时周期性地在 signal channel 检查是否有结束的请求。\n不可响应的 goroutine\n1. 尽量使用 Non-blocking IO (正如 go runtime 那样)\n2. 尽量使用阻塞粒度较小的 sys calls (对外部调用也一样)\n3. 业务逻辑总是考虑退出机制，编码时避免潜在的死循环\n4. 在合适的地方插入响应 channel 的代码，保持一定频率的 channel 响应能力\n使用 context GO Context blog\nGO Context pkg\n对上面两篇文章的整理翻译。\ncontext 对一个 Go 服务，处理传入请求时应该创建一个Context，外部调用时应该接受一个Context。它们间的函数调用链必须传递Context，传递的 Context 也可以是使用WithCancel, WithDeadline, WithTimeout, or WithValue创建的继承来的Context。当一个Context被取消，所有继承它的Context也都会取消。\n// A Context carries a deadline, cancelation signal, and request-scoped values // across API boundaries. Its methods are safe for simultaneous use by multiple // goroutines. type Context interface { // Done returns a channel that is closed when this Context is canceled // or times out. Done() \u0026lt;-chan struct{} // Err indicates why this context was canceled, after the Done channel // is closed. Err() error // Deadline returns the time when this Context will be canceled, if any. Deadline() (deadline time.Time, ok bool) // Value returns the value associated with key or nil if none. Value(key interface{}) interface{} }   Done 返回一个只读信道（channel），它是表示 Context 是否已关闭(cancel)的信号。\n Err 返回Context被关闭的原因。\n Deadline 让方法可以决定是否应该开始工作，如果剩下的时间太少，可能不需要运行。也可以使用 deadline 来设置IO操作的超时时间。\n Value 方法允许Context绑定一个请求范围内(request-scoped)的数据。这个数据一定是线程安全的。\n  Context没有 cancel 方法和Done 信道是只读的原因一样：接收关闭信号(signal)的方法(function)通常不是发送信号的方法，尤其是，当父操作为子操作启动 goroutine 时，这些子操作的 goroutine 不应该能够关闭父操作。相反，WithCancel方法提供了关闭新Context的方式。\n多个 goroutine 同时使用一个Context是安全的。代码可以将单个Context传递给任意数量的 goroutine，并关闭该Conetxt以向所有这些 goroutine 发出信号。\nDerived contexts context包提供了从现有Context中继承新的Context的方法。这些Context构成一个树：当一个Context被关闭(cancel)时，继承自它的所有Context都会被关闭。\nBackground 是所有 Context 树的根，它永远不会关闭(cancel)：\n// Background returns an empty Context. It is never canceled, has no deadline, // and has no values. Background is typically used in main, init, and tests, // and as the top-level Context for incoming requests. func Background() Context  WithCancel和WithTimeout返回派生的Context，这些值可以比父Context更早取消。通常在请求处理程序返回时关闭与传入请求相关联的Context。WithCancel对于在使用多个副本时关闭冗余请求很有用。WithTimeout对设置后端服务器请求的截止日期时很有用：\n/ WithCancel returns a copy of parent whose Done channel is closed as soon as // parent.Done is closed or cancel is called. func WithCancel(parent Context) (ctx Context, cancel CancelFunc) // A CancelFunc cancels a Context. type CancelFunc func() // WithTimeout returns a copy of parent whose Done channel is closed as soon as // parent.Done is closed, cancel is called, or timeout elapses. The new // Context's Deadline is the sooner of now+timeout and the parent's deadline, if // any. If the timer is still running, the cancel function releases its // resources. func WithTimeout(parent Context, timeout time.Duration) (Context, CancelFunc)  WithValue提供了一种将请求范围的值与Context绑定的方法：\n// WithValue returns a copy of parent whose Value method returns val for key. func WithValue(parent Context, key interface{}, val interface{}) Context  使用原则 Programs that use Contexts should follow these rules to keep interfaces consistent across packages and enable static analysis tools to check context propagation:\n使用Context的程序包需要遵循如下的原则来满足接口的一致性以及便于静态分析:\nDo not store Contexts inside a struct type; instead, pass a Context explicitly to each function that needs it. The Context should be the first parameter, typically named ctx\n不要把 Context 存在一个结构体当中，显式地传入函数。Context变量需要作为第一个参数使用，一般命名为ctx\nDo not pass a nil Context, even if a function permits it. Pass context.TODO if you are unsure about which Context to use\n即使方法允许，也不要传入一个 nil 的 Context，如果你不确定你要用什么 Context 的时候传一个 context.TODO\nUse context Values only for request-scoped data that transits processes and APIs, not for passing optional parameters to functions\n使用context的Value相关方法只应该用于在程序和接口中传递的和请求相关的元数据，不要用它来传递一些可选的参数\nThe same Context may be passed to functions running in different goroutines; Contexts are safe for simultaneous use by multiple goroutines.\n同样的Context可以用来传递到不同的goroutine中，Context在多个goroutine中是安全的。\n"
},
{
	"uri": "/cloud/apiserver-%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/",
	"title": "Apiserver 源码阅读",
	"tags": [],
	"description": "",
	"content": "k8s apiserver 源码阅读笔记\n代码结构 本部分用于记录 apiserver 代码整体结构及关键方法，便于到源码中查找，个人阅读记录，读者可跳过。本文所有代码均基于 kubernetes 1.9.6。  app.Run() CreateServerChain() CreateNodeDialer()\t//ssh 连接 CreateKubeAPIServerConfig()\t// defaultOptions() DefaultAdvertiseAddress() DefaultServiceIPRange() MaybeDefaultWithSelfSignedCerts() ApplyAuthorization() IsValidServiceAccountKeyFile() Validate() //validate options BuildGenericConfig()\t//takes the master server options and produces the genericapiserver.Config associated with it genericConfig genericConfig.OpenAPIConfig //config swagger BuildStorageFactory()\t//constructs the storage factory func NewStorageFactory(storageConfig storagebackend.Config, defaultMediaType string, serializer runtime.StorageSerializer, defaultResourceEncoding *serverstorage.DefaultResourceEncodingConfig, storageEncodingOverrides map[string]schema.GroupVersion, resourceEncodingOverrides []schema.GroupVersionResource, defaultAPIResourceConfig *serverstorage.ResourceConfig, resourceConfigOverrides utilflag.ConfigurationMap) (*serverstorage.DefaultStorageFactory, error) // storageConfig -\u0026gt; ETCD配置 // defaultAPIResourceConfig -\u0026gt; func DefaultAPIResourceConfigSource() *serverstorage.ResourceConfig type ResourceConfig struct { GroupVersionResourceConfigs map[schema.GroupVersion]*GroupVersionResourceConfig } func (o *ResourceConfig) EnableVersions(versions ...schema.GroupVersion)\t//Enable GroupVersion func (o *ResourceConfig) EnableResources(resources ...schema.GroupVersionResource) //daemonsets,deployments,ingresses,networkpolicies,replicasets,podsecuritypolicies // Specifies the overrides for various API group versions. // This can be used to enable/disable entire group versions or specific resources. type GroupVersionResourceConfig struct { // Whether to enable or disable this entire group version. This dominates any enablement check. // Enable=true means the group version is enabled, and EnabledResources/DisabledResources are considered. // Enable=false means the group version is disabled, and EnabledResources/DisabledResources are not considered. Enable bool // DisabledResources lists the resources that are specifically disabled for a group/version // DisabledResources trumps EnabledResources DisabledResources sets.String // EnabledResources lists the resources that should be enabled by default. This is a little // unusual, but we need it for compatibility with old code for now. An empty set means // enable all, a non-empty set means that all other resources are disabled. EnabledResources sets.String } EtcdServersOverrides //override etcd配置 client, err := internalclientset.NewForConfig(genericConfig.LoopbackClientConfig) // new a loopback client sharedInformers := informers.NewSharedInformerFactory(client, 10*time.Minute)\t// SharedInformerFactory provides shared informers for resources in all known API group versions. type SharedInformerFactory interface { internalinterfaces.SharedInformerFactory ForResource(resource schema.GroupVersionResource) (GenericInformer, error) WaitForCacheSync(stopCh \u0026lt;-chan struct{}) map[reflect.Type]bool Admissionregistration() admissionregistration.Interface Apps() apps.Interface Autoscaling() autoscaling.Interface Batch() batch.Interface Certificates() certificates.Interface Core() core.Interface Extensions() extensions.Interface Networking() networking.Interface Policy() policy.Interface Rbac() rbac.Interface Scheduling() scheduling.Interface Settings() settings.Interface Storage() storage.Interface } serviceResolver // A ServiceResolver knows how to get a URL given a service. type ServiceResolver interface { ResolveEndpoint(namespace, name string) (*url.URL, error) } DefaultServiceIPRange() BuildStorageFactory() readCAorNil()\t//auth CA config := \u0026amp;master.Config{} type Config struct { GenericConfig *genericapiserver.Config ExtraConfig ExtraConfig } createAPIExtensionsConfig() createAPIExtensionsServer() // apiextensions-apiserver\\pkg\\apiserver\\apiserver.go New() is the function for CustomResourceDefinitions to register router handler on GenericAPIServer CreateKubeAPIServer() // creates and wires a workable kube-apiserver, // returns a new instance of Master from the given config func (c completedConfig) New(delegationTarget genericapiserver.DelegationTarget) (*Master, error){} // goproject\\src\\k8s.io\\kubernetes\\pkg\\master\\master.go c.GenericConfig.New(\u0026quot;kube-apiserver\u0026quot;, delegationTarget)\t// creates a new server which logically combines the handling chain with the passed server !! important apiServerHandler := NewAPIServerHandler(name, c.RequestContextMapper, c.Serializer, handlerChainBuilder, delegationTarget.UnprotectedHandler()) // k8s.io\\apiserver\\pkg\\server\\handler.go construct gorestfulContainer and add default handler(NotFoundHandler, RecoverHandler, ServiceErrorHandler) func NewAPIServerHandler(name string, contextMapper request.RequestContextMapper, s runtime.NegotiatedSerializer, handlerChainBuilder HandlerChainBuilderFn, notFoundHandler http.Handler) *APIServerHandler {} director := director{...} type director struct { name string goRestfulContainer *restful.Container nonGoRestfulMux *mux.PathRecorderMux } func (d director) ServeHTTP(w http.ResponseWriter, req *http.Request) {} s := \u0026amp;GenericAPIServer{}\t// k8s.io\\apiserver\\pkg\\server\\genericapiserver.go type GenericAPIServer add PostStartHooks \u0026amp; PreShutdownHooks AddPostStartHook(PostStartHookFunc)\t//\t{c.SharedInformerFactory.Start(context.StopCh)} add healthzChecks installAPI(s, c.Config)\t// install utils routes like SwaggerUI, Profiling, Metrics if () routes.DefaultMetrics{}.Install(s.Handler.NonGoRestfulMux) if () routes.Logs{}.Install(s.Handler.GoRestfulContainer) m := \u0026amp;Master{ GenericAPIServer: s, } m.InstallLegacyAPI(\u0026amp;c, c.GenericConfig.RESTOptionsGetter, legacyRESTStorageProvider)\t// k8s.io\\kubernetes\\pkg\\master\\master.go, install core api routes, !!!!important NewLegacyRESTStorage() -\u0026gt; //定义如下 返回 LegacyRESTStorage和APIGroupInfo, Storage保存了具体资源对象的结构，如 PodStrorage // LegacyRESTStorage returns stateful information about particular instances of REST storage to master.go for wiring controllers func (c LegacyRESTStorageProvider) NewLegacyRESTStorage(restOptionsGetter generic.RESTOptionsGetter) (LegacyRESTStorage, genericapiserver.APIGroupInfo, error) {} // k8s.io\\kubernetes\\pkg\\registry\\core\\rest\\storage_core.go // Info about an API group. type APIGroupInfo struct { GroupMeta apimachinery.GroupMeta // Info about the resources in this group. Its a map from version to resource to the storage. VersionedResourcesStorageMap map[string]map[string]rest.Storage\t// !!! important!!!! // OptionsExternalVersion controls the APIVersion used for common objects in the // schema like api.Status, api.DeleteOptions, and metav1.ListOptions. Other implementors may // define a version \u0026quot;v1beta1\u0026quot; but want to use the Kubernetes \u0026quot;v1\u0026quot; internal objects. // If nil, defaults to groupMeta.GroupVersion. // TODO: Remove this when https://github.com/kubernetes/kubernetes/issues/19018 is fixed. OptionsExternalVersion *schema.GroupVersion // MetaGroupVersion defaults to \u0026quot;meta.k8s.io/v1\u0026quot; and is the scheme group version used to decode // common API implementations like ListOptions. Future changes will allow this to vary by group // version (for when the inevitable meta/v2 group emerges). MetaGroupVersion *schema.GroupVersion // Scheme includes all of the types used by this group and how to convert between them (or // to convert objects from outside of this group that are accepted in this API). // TODO: replace with interfaces Scheme *runtime.Scheme // NegotiatedSerializer controls how this group encodes and decodes data NegotiatedSerializer runtime.NegotiatedSerializer // ParameterCodec performs conversions for query parameters passed to API calls ParameterCodec runtime.ParameterCodec } restStorage := LegacyRESTStorage{} // NewREST for basic resources podTemplateStorage := podtemplatestore.NewREST(restOptionsGetter) eventStorage := eventstore.NewREST(restOptionsGetter, uint64(c.EventTTL.Seconds())) limitRangeStorage := limitrangestore.NewREST(restOptionsGetter) resourceQuotaStorage, resourceQuotaStatusStorage := resourcequotastore.NewREST(restOptionsGetter) secretStorage := secretstore.NewREST(restOptionsGetter) serviceAccountStorage := serviceaccountstore.NewREST(restOptionsGetter) persistentVolumeStorage, persistentVolumeStatusStorage := pvstore.NewREST(restOptionsGetter) persistentVolumeClaimStorage, persistentVolumeClaimStatusStorage := pvcstore.NewREST(restOptionsGetter) configMapStorage := configmapstore.NewREST(restOptionsGetter) namespaceStorage, namespaceStatusStorage, namespaceFinalizeStorage := namespacestore.NewREST(restOptionsGetter) endpointsStorage := endpointsstore.NewREST(restOptionsGetter) endpointRegistry := endpoint.NewRegistry(endpointsStorage) podStorage := podstore.NewStorage() serviceRESTStorage, serviceStatusStorage := servicestore.NewREST(restOptionsGetter) serviceRegistry := service.NewRegistry(serviceRESTStorage) restStorageMap := map[string]rest.Storage{ \u0026quot;pods\u0026quot;: podStorage.Pod, \u0026quot;pods/attach\u0026quot;: podStorage.Attach, // ............... \u0026quot;configMaps\u0026quot;: configMapStorage, \u0026quot;componentStatuses\u0026quot;: componentstatus.NewStorage(componentStatusStorage{c.StorageFactory}.serversToValidate), } apiGroupInfo.VersionedResourcesStorageMap[\u0026quot;v1\u0026quot;] = restStorageMap // set APIGroupInfo.VersionedResourcesStorageMap to return, !!!!! // construct BootstrapController and add hook NewBootstrapController()\t// a controller for watching the core capabilities of the master AddPostStartHookOrDie() // { controller.start() } AddPreShutdownHookOrDie() // { controller.stop() } InstallLegacyAPIGroup() func (s *GenericAPIServer) InstallLegacyAPIGroup(apiPrefix string, apiGroupInfo *APIGroupInfo) error {} if legacyAPIGroupPrefixes.Has(apiPrefix) installAPIResources()\t// a private method for installing the REST storage backing each api groupversionresource for apiGroupInfo.GroupMeta.GroupVersions { // get rest.Storage from apiGroupInfo.VersionedResourcesStorageMap[groupVersion.Version] apiGroupVersion := s.getAPIGroupVersion(apiGroupInfo, groupVersion, apiPrefix) // InstallREST registers the REST handlers (storage, watch, proxy and redirect) into a restful Container apiGroupVersion.InstallREST(s.Handler.GoRestfulContainer) installer := \u0026amp;APIInstaller{ group: g,...} installer.Install() // Install handlers for API resources. k8s.io\\apiserver\\pkg\\endpoints\\installer.go func (a *APIInstaller) Install() ([]metav1.APIResource, *restful.WebService, []error) {} paths := make([]string, len(a.group.Storage)) for paths(storages) { apiResource, err := a.registerResourceHandlers(path, a.group.Storage[path], ws, proxyHandler)\t// !!important(700+ lines...), function to actually add route to go-restful. k8s.io\\apiserver\\pkg\\endpoints\\installer.go // kubernetes把所有对资源对象的操作接口封装到一个action对象中，在 registerResourceHandlers 方法中， // 根据如 storage.(rest.Getter)的方法，获取 what verbs are supported by the storage, used to know what verbs we support per path， // 根据scope: RESTScopeNameRoot(Handle non-namespace scoped resources like nodes) 或 RESTScopeNameNamespace(Handler for standard REST verbs (GET, PUT, POST and DELETE)) // 将 action append 到一个 actions 切片中(不同 scope ，path前缀不同) // 最后遍历actions，根据不同的action.Verb，注册到go-restful的 restful.WebService中，并将此对象支持的Verbs将入到apiResource.Verbs中并返回apiResource对象。 func (a *APIInstaller) registerResourceHandlers(path string, storage rest.Storage, ws *restful.WebService, proxyHandler http.Handler) (*metav1.APIResource, error) {} // Struct capturing information about an action (\u0026quot;GET\u0026quot;, \u0026quot;POST\u0026quot;, \u0026quot;WATCH\u0026quot;, \u0026quot;PROXY\u0026quot;, etc). type action struct { Verb string // Verb identifying the action (\u0026quot;GET\u0026quot;, \u0026quot;POST\u0026quot;, \u0026quot;WATCH\u0026quot;, \u0026quot;PROXY\u0026quot;, etc). Path string // The path of the action Params []*restful.Parameter // List of parameters associated with the action. Namer handlers.ScopeNamer AllNamespaces bool // true iff the action is namespaced but works on aggregate result for all namespaces } // APIResource specifies the name of a resource and whether it is namespaced. type APIResource struct { // name is the plural name of the resource. Name string `json:\u0026quot;name\u0026quot; protobuf:\u0026quot;bytes,1,opt,name=name\u0026quot;` // singularName is the singular name of the resource. This allows clients to handle plural and singular opaquely. // The singularName is more correct for reporting status on a single item and both singular and plural are allowed // from the kubectl CLI interface. SingularName string `json:\u0026quot;singularName\u0026quot; protobuf:\u0026quot;bytes,6,opt,name=singularName\u0026quot;` // namespaced indicates if a resource is namespaced or not. Namespaced bool `json:\u0026quot;namespaced\u0026quot; protobuf:\u0026quot;varint,2,opt,name=namespaced\u0026quot;` // group is the preferred group of the resource. Empty implies the group of the containing resource list. // For subresources, this may have a different value, for example: Scale\u0026quot;. Group string `json:\u0026quot;group,omitempty\u0026quot; protobuf:\u0026quot;bytes,8,opt,name=group\u0026quot;` // version is the preferred version of the resource. Empty implies the version of the containing resource list // For subresources, this may have a different value, for example: v1 (while inside a v1beta1 version of the core resource's group)\u0026quot;. Version string `json:\u0026quot;version,omitempty\u0026quot; protobuf:\u0026quot;bytes,9,opt,name=version\u0026quot;` // kind is the kind for the resource (e.g. 'Foo' is the kind for a resource 'foo') Kind string `json:\u0026quot;kind\u0026quot; protobuf:\u0026quot;bytes,3,opt,name=kind\u0026quot;` // verbs is a list of supported kube verbs (this includes get, list, watch, create, // update, patch, delete, deletecollection, and proxy) Verbs Verbs `json:\u0026quot;verbs\u0026quot; protobuf:\u0026quot;bytes,4,opt,name=verbs\u0026quot;` // shortNames is a list of suggested short names of the resource. ShortNames []string `json:\u0026quot;shortNames,omitempty\u0026quot; protobuf:\u0026quot;bytes,5,rep,name=shortNames\u0026quot;` // categories is a list of the grouped resources this resource belongs to (e.g. 'all') Categories []string `json:\u0026quot;categories,omitempty\u0026quot; protobuf:\u0026quot;bytes,7,rep,name=categories\u0026quot;` } apiResources = append(apiResources, *apiResource) } } m.InstallAPIs(c.ExtraConfig.APIResourceConfigSource, c.GenericConfig.RESTOptionsGetter, restStorageProviders...)\t// InstallAPIs will install the APIs for the restStorageProviders if they are enabled. for restStorageProviders { apiGroupsInfo = append(apiGroupsInfo, apiGroupInfo) } for i := range apiGroupsInfo { m.GenericAPIServer.InstallAPIGroup(\u0026amp;apiGroupsInfo[i]) // installAPIResources is a private method for installing the REST storage backing each api groupversionresource, k8s.io\\apiserver\\pkg\\server\\genericapiserver.go func (s *GenericAPIServer) installAPIResources(apiPrefix string, apiGroupInfo *APIGroupInfo) error {} // ！！InstallREST registers the REST handlers (storage, watch, proxy and redirect) into a restful Container, k8s.io\\apiserver\\pkg\\endpoints\\groupversion.go apiGroupVersion.InstallREST(s.Handler.GoRestfulContainer } m.installTunneler(c.ExtraConfig.Tunneler, corev1client.NewForConfigOrDie(c.GenericConfig.LoopbackClientConfig).Nodes()) m.GenericAPIServer.AddPostStartHookOrDie(\u0026quot;ca-registration\u0026quot;, c.ExtraConfig.ClientCARegistrationHook.PostStartHook) AddPostStartHook()\t// addfunc {sharedInformers.Start(context.StopCh)} // openapi swagger // this wires up openapi kubeAPIServer.GenericAPIServer.PrepareRun() // This will wire up openapi for extension api server apiExtensionsServer.GenericAPIServer.PrepareRun() aggregatorConfig, err := createAggregatorConfig(*kubeAPIServerConfig.GenericConfig, runOptions, versionedInformers, serviceResolver, proxyTransport) aggregatorServer, err := createAggregatorServer(aggregatorConfig, kubeAPIServer.GenericAPIServer, apiExtensionsServer.Informers) aggregatorServer, err := aggregatorConfig.Complete().NewWithDelegate(delegateAPIServer) genericServer, err := c.GenericConfig.New(\u0026quot;kube-aggregator\u0026quot;, delegationTarget) // ----same function with called in CreateKubeAPIServer-\u0026gt;New() (91) apiregistrationClient, err := internalclientset.NewForConfig(c.GenericConfig.LoopbackClientConfig) configShallowCopy.RateLimiter = flowcontrol.NewTokenBucketRateLimiter(configShallowCopy.QPS, configShallowCopy.Burst) cs.apiregistration, err = apiregistrationinternalversion.NewForConfig(\u0026amp;configShallowCopy) cs.DiscoveryClient, err = discovery.NewDiscoveryClientForConfig(\u0026amp;configShallowCopy) informerFactory := informers.NewSharedInformerFactory(apiregistrationClient,5*time.Minute,) s := \u0026amp;APIAggregator{}\t// APIAggregator contains state for a Kubernetes cluster master/api server. v1beta1storage := map[string]rest.Storage{} apiServiceREST := apiservicestorage.NewREST(Scheme, c.GenericConfig.RESTOptionsGetter) v1beta1storage[\u0026quot;apiservices\u0026quot;] = apiServiceREST v1beta1storage[\u0026quot;apiservices/status\u0026quot;] = apiservicestorage.NewStatusREST(Scheme, apiServiceREST) // rest implements a RESTStorage for API services against etcd type REST struct { *genericregistry.Store } apiGroupInfo.VersionedResourcesStorageMap[\u0026quot;v1beta1\u0026quot;] = v1beta1storage s.GenericAPIServer.InstallAPIGroup(\u0026amp;apiGroupInfo)\t// ----same function with called in CreateKubeAPIServer-\u0026gt;New()-\u0026gt;InstallAPIs() (237) s.GenericAPIServer.Handler.NonGoRestfulMux.Handle(\u0026quot;/apis\u0026quot;, apisHandler) s.GenericAPIServer.Handler.NonGoRestfulMux.UnlistedHandle(\u0026quot;/apis/\u0026quot;, apisHandler) apiserviceRegistrationController := NewAPIServiceRegistrationController(informerFactory.Apiregistration().InternalVersion().APIServices(), c.GenericConfig.SharedInformerFactory.Core().V1().Services(), s) // add event handler to informer s.GenericAPIServer.AddPostStartHook(\u0026quot;start-kube-aggregator-informers\u0026quot;) s.GenericAPIServer.AddPostStartHook(\u0026quot;apiservice-registration-controller\u0026quot;) s.GenericAPIServer.AddPostStartHook(\u0026quot;apiservice-status-available-controller\u0026quot;) // BuildAndRegisterAggregator registered OpenAPI aggregator handler. openAPIAggregator, err := openapicontroller.BuildAndRegisterAggregator() // create controllers for auto-registration apiRegistrationClient, err := apiregistrationclient.NewForConfig(aggregatorConfig.GenericConfig.LoopbackClientConfig) // ......  app.Run kubernetes所有组件的入口，基本上都是在$GOPATH\\k8s.io\\kubernetes\\cmd\\xxx(组件名称)下面的main文件中。 apiserver对应的路径为$GOPATH\\k8s.io\\kubernetes\\cmd\\kube-apiserver\\apiserver.go，main函数中通过app.Run(s, stopCh)方法，执行具体逻辑。 具体Run方法，定义在$GOPATH\\k8s.io\\kubernetes\\cmd\\kube-apiserver\\app\\server.go中。\napiserver的app.Run()，主要通过 CreateServerChain() 方法，创建出一个*genericapiserver.GenericAPIServer实例。\n在GenericAPIServer中，包含的主要结构体有\n *APIServerHandler(Handler holds the handlers being used by this API server)\n DelegationTarget(delegationTarget is the next delegate in the chain or nil)\n其中最重要的是APIServerHandler这个结构体，它包含了go-restful中的*restful.Container结构体，后面注册API时用到的InstallAPIs()方法，最终也是将路由注册到这个Container中，定义如下: golang // APIServerHandlers holds the different http.Handlers used by the API server. // This includes the full handler chain, the director (which chooses between gorestful and nonGoRestful, // the gorestful handler (used for the API) which falls through to the nonGoRestful handler on unregistered paths, // and the nonGoRestful handler (which can contain a fallthrough of its own) // FullHandlerChain -\u0026gt; Director -\u0026gt; {GoRestfulContainer,NonGoRestfulMux} based on inspection of registered web services type APIServerHandler struct { // FullHandlerChain is the one that is eventually served with. It should include the full filter // chain and then call the Director. FullHandlerChain http.Handler // The registered APIs. InstallAPIs uses this. Other servers probably shouldn't access this directly. GoRestfulContainer *restful.Container // NonGoRestfulMux is the final HTTP handler in the chain. // It comes after all filters and the API handling // This is where other servers can attach handler to various parts of the chain. NonGoRestfulMux *mux.PathRecorderMux // Other servers should only use this opaquely to delegate to an API server. Director http.Handler }  DelegationTarget(DelegationTarget is an interface which allows for composition of API servers with top level handling that works as expected.)是一个interface，是构成方法名CreateServerChain中ServerChain的结构，结构体内定义了NextDelegate()方法，返回chain中的下一个DelegationTarget，由它串起了多个api servers。(为什么会有多个api server从后面代码中可以看到。)\n  CreateServerChain // CreateServerChain creates the apiservers connected via delegation. func CreateServerChain(runOptions *options.ServerRunOptions, stopCh \u0026lt;-chan struct{}) (*genericapiserver.GenericAPIServer, error) {}  CreateServerChain()方法中，先后执行了CreateNodeDialer, CreateKubeAPIServerConfig, createAPIExtensionsConfig, createAPIExtensionsServer, CreateKubeAPIServer, createAggregatorConfig, createAggregatorServer几个方法，根据方法名可以看出启动apiserver的流程。\n CreateNodeDialer(CreateNodeDialer creates the dialer infrastructure to connect to the nodes), add SSH Key, 返回一个tunneler.Tunneler, 可以通过创建到node节点的SSH连接。\n CreateKubeAPIServerConfig(creates all the resources for running the API server, but runs none of them), 创建出所有apiserver所需的配置和资源，包括配置的Validate，命令行参数解析，openapi/swagger配置，StorageFactory,clientset, informer, serviceResolver 等资源的创建。\n createAPIExtensionsConfig, 传入由上一步生成的配置*kubeAPIServerConfig.GenericConfig和 informer, 通过apiextensionscmd.NewCRDRESTOptionsGetter(etcdOptions)初始化ExtraConfig.CRDRESTOptionsGetter并创建 apiextensionsConfig 返回。\n createAPIExtensionsServer, 通过上一步生成的apiExtensionsConfig，通过一个genericapiserver.EmptyDelegate创建 apiExtensionsServer。返回 apiserver 的是一个apiextensionsapiserver.CustomResourceDefinitions结构体。其中生成 CustomResourceDefinitions 结构体的 New() 方法，真正将 CRD 接口添加到apiGroupInfo.VersionedResourcesStorageMap中，并注册到 go-resetful 的 webService，同时会通过AddPostStartHook添加启动后hook，启动informer(事件监听)和crdController, namingController, finalizingController三个 Controler 监听 CRD Resource 的变化。\n CreateKubeAPIServer(creates and wires a workable kube-apiserver), 通过以上几步生成的 apiserver 配置，通过createAPIExtensionsServer生成的DelegationTarget创建 apiserver 实例(master.Master)。这个过程中会 install kubernetes 的 core api 并 启动 BootStrapController(a controller for watching the core capabilities of the master), install nodeTunneler 并添加ca-registration的 PostStartHook。\n createAggregatorConfig, 通过上面生成的 apiserver 配置生成 AggregatorConfig。(代码中只是浅拷贝一份kubeAPIServerConfig.GenericConfig并添加了 Proxy 相关的 ExtraConfig 到返回的*aggregatorapiserver.Config结构体中。)\n createAggregatorServer, 生成 AggregatorServer(*Aggregator for Kubernetes-style API servers: dynamic registration, discovery summarization, secure proxy *)。这个过程中，会启动apiserviceRegistrationController, availableController 去监听 api service 资源，完成 api service 的发现和注册。\n  这里解释一下 aggregator, 这是 kubernetes 为了增强 apiserver 的扩展性，方便用户开发自己的 api服务而开发的机制。它允许k8s的开发人员编写一个自己的服务，可以把这个服务注册到k8s的api里面，这样，就像k8s自己的api一样，你的服务只要运行在k8s集群里面，k8s 的Aggregate通过service名称就可以转发到你写的service里面去了。\n\u0026gt;Aggregated（聚合的）API server是为了将原来的API server这个巨石（monolithic）应用给拆分成，为了方便用户开发自己的API server集成进来，而不用直接修改kubernetes官方仓库的代码，这样一来也能将API server解耦，方便用户使用实验特性。这些API server可以跟core API server无缝衔接，使用kubectl也可以管理它们。\n到这里， apiserver的启动就基本完成了。\n下面主要分析下以上几个流程中CreateKubeAPIServerConfig和CreateKubeAPIServer两个方法，这也是创建出核心的apiserver 和真正执行k8s core api 注册的过程。\nCreateKubeAPIServerConfig kubeAPIServerConfig, sharedInformers, versionedInformers, insecureServingOptions, serviceResolver, err := CreateKubeAPIServerConfig(runOptions, nodeTunneler, proxyTransport) // CreateKubeAPIServerConfig creates all the resources for running the API server, but runs none of them func CreateKubeAPIServerConfig(s *options.ServerRunOptions, nodeTunneler tunneler.Tunneler, proxyTransport *http.Transport) (*master.Config, informers.SharedInformerFactory, clientgoinformers.SharedInformerFactory, *kubeserver.InsecureServingInfo, aggregatorapiserver.ServiceResolver, error) {}  方法的注释是 创建为了运行API server所需的所有资源，但是不会运行。就是说，这个方法负责创建后面启动的apiserver所需的所有配置及相关类的初始化。通过返回参数看，这些资源至少包括apiserver的配置, SharedInformerFactory(provides shared informers for resources in all known API group versions), InsecureServingInfo(is required to serve http. HTTP does NOT include authentication or authorization.), ServiceResolver(knows how to get a URL given a service)。\n主要流程 CreateKubeAPIServerConfig 首先是通过 defaultOptions 方法 在创建真正的 apiserver配置前将 options 中的参数以默认值补全，并对参数进行Validate, 然后通过BuildGenericConfig方法，根据 options 创建*genericapiserver.Config, 同时 SharedInformer和ServiceResolver都是在这个方法中创建的。 BuildGenericConfig方法中调用了很多ApplyTo方法，作用是将 options 中的各项配置参数解析到生成的config中, 在这个方法中还创建了[]admission.PluginInitializer。\n在这之后还有一个重要的方法是BuildStorageFactory，创建StorageFactory的时候需要传入 etcd 相关的配置。Storage是apiserver中一个很重要的概念，通过它执行对具体资源对象的操作，如对 POD 的 CRUD 等操作就是通过 PodStorage对象进行并连接到后端的 etcd 的。（同时 Storage 也是和 对应资源对象的 API 对应，后面installAPI的时候也是通过 Storage 来注册 API 路由的。）\n最后，配置默认的ServiceIPRange 和 获取 CA 证书等配置，将上面创建的配置注入一个\u0026amp;master.Config实例并返回。\nServiceResolver ServiceResolver的定义及创建方法如下，通过 SharedInformer 的 lister 方法监听 kubernetes Service 资源的变化，实现获取 Service 的 URL 的功能。\nserviceResolver = aggregatorapiserver.NewClusterIPServiceResolver( versionedInformers.Core().V1().Services().Lister(), ) // A ServiceResolver knows how to get a URL given a service. type ServiceResolver interface { ResolveEndpoint(namespace, name string) (*url.URL, error) }  SharedInformer CreateKubeAPIServerConfig 返回两个SharedInformerFactory, 实际上结构体的定义完全相同，区别是定义在不同的包内，informers.SharedInformerFactory定义在 kubernetes 内部的pkg下，而clientgoinformers.SharedInformerFactory定义在 client-go 中，因此创建的时候，前者是通过internalclientset创建出的clientset创建，而后者是通过client-go的clientset创建，用来创建两者的配置是完全相同的。(第一次阅读，比较疑惑为什么需要两个clientset。猜测是一个用来内部通信，一个是用来外部通信。后面有时间的话，会再具体研究下。)\nSharedInformerFactory定义如下：\n// SharedInformerFactory provides shared informers for resources in all known // API group versions. type SharedInformerFactory interface { internalinterfaces.SharedInformerFactory ForResource(resource schema.GroupVersionResource) (GenericInformer, error) WaitForCacheSync(stopCh \u0026lt;-chan struct{}) map[reflect.Type]bool Admissionregistration() admissionregistration.Interface Apps() apps.Interface Autoscaling() autoscaling.Interface Batch() batch.Interface Certificates() certificates.Interface Core() core.Interface Extensions() extensions.Interface Networking() networking.Interface Policy() policy.Interface Rbac() rbac.Interface Scheduling() scheduling.Interface Settings() settings.Interface Storage() storage.Interface }  SharedInformerFactory为所有API group versions提供shared informers, shared informer又是什么呢？定义如下：\n// SharedInformer has a shared data cache and is capable of distributing notifications for changes // to the cache to multiple listeners who registered via AddEventHandler. If you use this, there is // one behavior change compared to a standard Informer. When you receive a notification, the cache // will be AT LEAST as fresh as the notification, but it MAY be more fresh. You should NOT depend // on the contents of the cache exactly matching the notification you've received in handler // functions. If there was a create, followed by a delete, the cache may NOT have your item. This // has advantages over the broadcaster since it allows us to share a common cache across many // controllers. Extending the broadcaster would have required us keep duplicate caches for each // watch. type SharedInformer interface { // AddEventHandler adds an event handler to the shared informer using the shared informer's resync // period. Events to a single handler are delivered sequentially, but there is no coordination // between different handlers. AddEventHandler(handler ResourceEventHandler) // AddEventHandlerWithResyncPeriod adds an event handler to the shared informer using the // specified resync period. Events to a single handler are delivered sequentially, but there is // no coordination between different handlers. AddEventHandlerWithResyncPeriod(handler ResourceEventHandler, resyncPeriod time.Duration) // GetStore returns the Store. GetStore() Store // GetController gives back a synthetic interface that \u0026quot;votes\u0026quot; to start the informer GetController() Controller // Run starts the shared informer, which will be stopped when stopCh is closed. Run(stopCh \u0026lt;-chan struct{}) // HasSynced returns true if the shared informer's store has synced. HasSynced() bool // LastSyncResourceVersion is the resource version observed when last synced with the underlying // store. The value returned is not synchronized with access to the underlying store and is not // thread-safe. LastSyncResourceVersion() string }  可以通过先两篇文章了解下SharedInformer, https://blog.csdn.net/weixin_42663840/article/details/81699303\nhttps://www.kubernetes.org.cn/2693.html\n简单来说，SharedInformer 有一个共享数据的cache, 并能够将 cache 的变化分发给多个 listener, 这些 listener 都是通过 AddEventHandler 方法注册到 SharedInformer。Informer 在初始化的时，先调用 Kubernetes List API 到 ETCD获得某种 resource 的全部 Object，缓存在内存中; 然后，调用 Watch API 去 watch 这种 resource，去维护这份缓存; 最后，Informer 就不再调用 Kubernetes 的任何 API。\nCreateKubeAPIServer kubeAPIServer, err := CreateKubeAPIServer(kubeAPIServerConfig, apiExtensionsServer.GenericAPIServer, sharedInformers, versionedInformers) // CreateKubeAPIServer creates and wires a workable kube-apiserver func CreateKubeAPIServer(kubeAPIServerConfig *master.Config, delegateAPIServer genericapiserver.DelegationTarget, sharedInformers informers.SharedInformerFactory, versionedInformers clientgoinformers.SharedInformerFactory) (*master.Master, error) {}  方法的注释 创建并装配一个可工作的 kube-apiserver, 到这里，一个真正可运行的apiserver实例就创建完成了。可以看到返回的 APIServer 类型是*master.Master，传入的就是之前CreateKubeAPIServer返回的配置和资源，加上delegateAPIServer。这是kubernetes组合多个 apiserver 的机制。\n主要流程 生成 kubeAPIServer 的是kubeAPIServerConfig.Complete(versionedInformers).New(delegateAPIServer)方法。在这个方法中，又调用了c.GenericConfig.New(\u0026quot;kube-apiserver\u0026quot;, delegationTarget)创建一个APIServer，并生成 Handler chain 和传入的delegateAPIServer组合起来；接着会新建*master.Master实例 m，并将c.GenericConfig.New 返回的 APIServer 赋值给 m.GenericAPIServer, 这个m也是CreateKubeAPIServer方法最终要返回的 APISever 实例。最后要做的就是执行m.InstallLegacyAPI, m.InstallAPIs注册 API 接口，添加 PostStartHook 然后将m返回。\nc.GenericConfig.New // New creates a new server which logically combines the handling chain with the passed server. // name is used to differentiate for logging. func (c completedConfig) New(name string, delegationTarget DelegationTarget) (*GenericAPIServer, error) {}  首先通过apiServerHandler := NewAPIServerHandler(name, c.RequestContextMapper, c.Serializer, handlerChainBuilder, delegationTarget.UnprotectedHandler())创建出一个APIServerHandler实例，结构体定义上面已经贴过了。\nNewAPIServerHandler 方法中，创建了nonGoRestfulMux和gorestfulContainer, 并给gorestfulContainer添加了几个默认Handler(NotFoundHandler, RecoverHandler, ServiceErrorHandler), 再这两者注入到一个 director 实例中，director 有一个 ServeHTTP, 用来最终启动 http 服务, 最后将director 赋值给 APIServerHandler.Director, 通过调用c.BuildHandlerChainFunc(director, c.Config)装饰director并赋值给APIServerHandler.FullHandlerChain最后返回。\n接着创建一个GenericAPIServer实例s，并将 NewAPIServerHandler 方法中返回的apiServerHandler赋值给s.Handler和s.listedPathProvider, 将传入的delegationTarget(即delegated apiserver) 中配置的 Hooks 和 HealthzCheckers 传递s, 并合并s和delegationTarget的 listedPathProvider(an interface for providing paths that should be reported at /), 最后执行installAPI(s, c.Config)安装 API 并返回s。\ndirector 如下是BuildHandlerChainFunc和*master.Config中默认的方法，可以看到是不断追加 handler 方法到 Handler 中。\n// BuildHandlerChainFunc allows you to build custom handler chains by decorating the apiHandler. BuildHandlerChainFunc func(apiHandler http.Handler, c *Config) (secure http.Handler) // default BuildHandlerChainFunc func DefaultBuildHandlerChain(apiHandler http.Handler, c *Config) http.Handler { handler := genericapifilters.WithAuthorization(apiHandler, c.RequestContextMapper, c.Authorizer, c.Serializer) handler = genericfilters.WithMaxInFlightLimit(handler, c.MaxRequestsInFlight, c.MaxMutatingRequestsInFlight, c.RequestContextMapper, c.LongRunningFunc) handler = genericapifilters.WithImpersonation(handler, c.RequestContextMapper, c.Authorizer, c.Serializer) if utilfeature.DefaultFeatureGate.Enabled(features.AdvancedAuditing) { handler = genericapifilters.WithAudit(handler, c.RequestContextMapper, c.AuditBackend, c.AuditPolicyChecker, c.LongRunningFunc) } else { handler = genericapifilters.WithLegacyAudit(handler, c.RequestContextMapper, c.LegacyAuditWriter) } failedHandler := genericapifilters.Unauthorized(c.RequestContextMapper, c.Serializer, c.SupportsBasicAuth) if utilfeature.DefaultFeatureGate.Enabled(features.AdvancedAuditing) { failedHandler = genericapifilters.WithFailedAuthenticationAudit(failedHandler, c.RequestContextMapper, c.AuditBackend, c.AuditPolicyChecker) } handler = genericapifilters.WithAuthentication(handler, c.RequestContextMapper, c.Authenticator, failedHandler) handler = genericfilters.WithCORS(handler, c.CorsAllowedOriginList, nil, nil, nil, \u0026quot;true\u0026quot;) handler = genericfilters.WithTimeoutForNonLongRunningRequests(handler, c.RequestContextMapper, c.LongRunningFunc, c.RequestTimeout) handler = genericfilters.WithWaitGroup(handler, c.RequestContextMapper, c.LongRunningFunc, c.HandlerChainWaitGroup) handler = genericapifilters.WithRequestInfo(handler, c.RequestInfoResolver, c.RequestContextMapper) handler = apirequest.WithRequestContext(handler, c.RequestContextMapper) handler = genericfilters.WithPanicRecovery(handler) return handler }  如下是director的定义及ServeHTTP方法，先匹配 path 是否是gorestful中的路径，是的话通过goRestfulContainer.Dispatch(w, req)分发到对应 handler 处理请求，不匹配的话就通过nonGoRestfulMux分发处理。\ntype director struct { name string goRestfulContainer *restful.Container nonGoRestfulMux *mux.PathRecorderMux } func (d director) ServeHTTP(w http.ResponseWriter, req *http.Request) { path := req.URL.Path // check to see if our webservices want to claim this path for _, ws := range d.goRestfulContainer.RegisteredWebServices() { switch { case ws.RootPath() == \u0026quot;/apis\u0026quot;: // if we are exactly /apis or /apis/, then we need special handling in loop. // normally these are passed to the nonGoRestfulMux, but if discovery is enabled, it will go directly. // We can't rely on a prefix match since /apis matches everything (see the big comment on Director above) if path == \u0026quot;/apis\u0026quot; || path == \u0026quot;/apis/\u0026quot; { glog.V(5).Infof(\u0026quot;%v: %v %q satisfied by gorestful with webservice %v\u0026quot;, d.name, req.Method, path, ws.RootPath()) // don't use servemux here because gorestful servemuxes get messed up when removing webservices // TODO fix gorestful, remove TPRs, or stop using gorestful d.goRestfulContainer.Dispatch(w, req) return } case strings.HasPrefix(path, ws.RootPath()): // ensure an exact match or a path boundary match if len(path) == len(ws.RootPath()) || path[len(ws.RootPath())] == '/' { glog.V(5).Infof(\u0026quot;%v: %v %q satisfied by gorestful with webservice %v\u0026quot;, d.name, req.Method, path, ws.RootPath()) // don't use servemux here because gorestful servemuxes get messed up when removing webservices // TODO fix gorestful, remove TPRs, or stop using gorestful d.goRestfulContainer.Dispatch(w, req) return } } } // if we didn't find a match, then we just skip gorestful altogether glog.V(5).Infof(\u0026quot;%v: %v %q satisfied by nonGoRestful\u0026quot;, d.name, req.Method, path) d.nonGoRestfulMux.ServeHTTP(w, req) }  ListedPathProvider // ListedPathProvider is an interface for providing paths that should be reported at /. type ListedPathProvider interface { // ListedPaths is an alphabetically sorted list of paths to be reported at /. ListedPaths() []string }  installAPI 这里的 installAPI 方法如下，只根据配置安装了 Index, SwaggerUI, Profiling, Metrics 等 API 到 NonGoRestfulMux, Version(/version) 到 GoRestfulContainer, 核心的 API 还没有安装。\nm.InstallLegacyAPI m.InstallLegacyAPI(\u0026amp;c, c.GenericConfig.RESTOptionsGetter, legacyRESTStorageProvider) func (m *Master) InstallLegacyAPI(c *completedConfig, restOptionsGetter generic.RESTOptionsGetter, legacyRESTStorageProvider corerest.LegacyRESTStorageProvider) {}  用于注册/api下的 API, 即core api。首先调用legacyRESTStorageProvider.NewLegacyRESTStorage创建legacyRESTStorage, 如果配置中EnableCoreControllers为True的话，创建BootStrapController并在 m的 PostStartHook 和 dPostStartHook 中添加启动和停止。最后执行m.GenericAPIServer.InstallLegacyAPIGroup安装 LegacyAPIGroup。\nNewLegacyRESTStorage legacyRESTStorage, apiGroupInfo, err := legacyRESTStorageProvider.NewLegacyRESTStorage(restOptionsGetter) func (c LegacyRESTStorageProvider) NewLegacyRESTStorage(restOptionsGetter generic.RESTOptionsGetter) (LegacyRESTStorage, genericapiserver.APIGroupInfo, error) {}  这个方法返回两个结构体，LegacyRESTStorage和genericapiserver.APIGroupInfo, 其中最重要的是 APIGroupInfo。APIGroupInfo 中，有一个VersionedResourcesStorageMap, 是API 版本到 Storage 资源的 map 关系，保存了不同版本的所有 Storage。下面贴出了两个结构体的定义。 首先会初始化一个 APIGroupInfo 实例apiGroupInfo, 接着会调用不同 Storage 资源的NewREST方法创建 Storage, 如: eventStorage, configMapStorage, namespaceStorage, serviceRESTStorage, podStorage等。NewREST方法返回一个 REST 结构体, 而 REST 结构体中，保存了*genericregistry.Store, 这个 Store 结构体提供了对应资源的 CRUD 等操作的方法，所有对资源的操作通过 Store 来访问到后端的 ETCD。其中 pod, node 和 service 对应的操作比较多，涉及 Status 的存储和更新, 所以 Storage 创建过程也较为复杂。尤其是 pod, 不仅涉及到本身和状态的存储和更新，还涉及到日志 proxy 等操作，所以单独封装了一个 PodStorage 结构体，其中包含了多种不同的 Store, 例如涉及到日志的LogREST需要传入 KubeletConnectionInfo, 涉及到 proxy 的 ProxyREST 需要传入 ProxyTransport等，每种 Store 都提供了对应资源的操作方法，如获取日志，建立连接等。这部分代码在k8s.io\\kubernetes\\pkg\\registry\\core\\rest\\storage_core.go, 有关 Pod 和 Service 的 Storage 创建及不同 Storage 对应的方法可以看一下。\n最后，所有 Storage 创建完后，会构建一个restStorageMap(具体内容会在下面贴出)，这个 map 最后会赋给apiGroupInfo.VersionedResourcesStorageMap[\u0026quot;v1\u0026quot;], 即 core API v1版本的所有资源都可以在这个 map 资源中找到。\n// LegacyRESTStorage returns stateful information about particular instances of REST storage to // master.go for wiring controllers. // TODO remove this by running the controller as a poststarthook type LegacyRESTStorage struct { ServiceClusterIPAllocator rangeallocation.RangeRegistry ServiceNodePortAllocator rangeallocation.RangeRegistry } // Info about an API group. type APIGroupInfo struct { GroupMeta apimachinery.GroupMeta // Info about the resources in this group. Its a map from version to resource to the storage. VersionedResourcesStorageMap map[string]map[string]rest.Storage // OptionsExternalVersion controls the APIVersion used for common objects in the // schema like api.Status, api.DeleteOptions, and metav1.ListOptions. Other implementors may // define a version \u0026quot;v1beta1\u0026quot; but want to use the Kubernetes \u0026quot;v1\u0026quot; internal objects. // If nil, defaults to groupMeta.GroupVersion. // TODO: Remove this when https://github.com/kubernetes/kubernetes/issues/19018 is fixed. OptionsExternalVersion *schema.GroupVersion // MetaGroupVersion defaults to \u0026quot;meta.k8s.io/v1\u0026quot; and is the scheme group version used to decode // common API implementations like ListOptions. Future changes will allow this to vary by group // version (for when the inevitable meta/v2 group emerges). MetaGroupVersion *schema.GroupVersion // Scheme includes all of the types used by this group and how to convert between them (or // to convert objects from outside of this group that are accepted in this API). // TODO: replace with interfaces Scheme *runtime.Scheme // NegotiatedSerializer controls how this group encodes and decodes data NegotiatedSerializer runtime.NegotiatedSerializer // ParameterCodec performs conversions for query parameters passed to API calls ParameterCodec runtime.ParameterCodec }  // restStorageMap restStorageMap := map[string]rest.Storage{ \u0026quot;pods\u0026quot;: podStorage.Pod, \u0026quot;pods/attach\u0026quot;: podStorage.Attach, \u0026quot;pods/status\u0026quot;: podStorage.Status, \u0026quot;pods/log\u0026quot;: podStorage.Log, \u0026quot;pods/exec\u0026quot;: podStorage.Exec, \u0026quot;pods/portforward\u0026quot;: podStorage.PortForward, \u0026quot;pods/proxy\u0026quot;: podStorage.Proxy, \u0026quot;pods/binding\u0026quot;: podStorage.Binding, \u0026quot;bindings\u0026quot;: podStorage.Binding, \u0026quot;podTemplates\u0026quot;: podTemplateStorage, \u0026quot;replicationControllers\u0026quot;: controllerStorage.Controller, \u0026quot;replicationControllers/status\u0026quot;: controllerStorage.Status, \u0026quot;services\u0026quot;: serviceRest.Service, \u0026quot;services/proxy\u0026quot;: serviceRest.Proxy, \u0026quot;services/status\u0026quot;: serviceStatusStorage, \u0026quot;endpoints\u0026quot;: endpointsStorage, \u0026quot;nodes\u0026quot;: nodeStorage.Node, \u0026quot;nodes/status\u0026quot;: nodeStorage.Status, \u0026quot;nodes/proxy\u0026quot;: nodeStorage.Proxy, \u0026quot;events\u0026quot;: eventStorage, \u0026quot;limitRanges\u0026quot;: limitRangeStorage, \u0026quot;resourceQuotas\u0026quot;: resourceQuotaStorage, \u0026quot;resourceQuotas/status\u0026quot;: resourceQuotaStatusStorage, \u0026quot;namespaces\u0026quot;: namespaceStorage, \u0026quot;namespaces/status\u0026quot;: namespaceStatusStorage, \u0026quot;namespaces/finalize\u0026quot;: namespaceFinalizeStorage, \u0026quot;secrets\u0026quot;: secretStorage, \u0026quot;serviceAccounts\u0026quot;: serviceAccountStorage, \u0026quot;persistentVolumes\u0026quot;: persistentVolumeStorage, \u0026quot;persistentVolumes/status\u0026quot;: persistentVolumeStatusStorage, \u0026quot;persistentVolumeClaims\u0026quot;: persistentVolumeClaimStorage, \u0026quot;persistentVolumeClaims/status\u0026quot;: persistentVolumeClaimStatusStorage, \u0026quot;configMaps\u0026quot;: configMapStorage, \u0026quot;componentStatuses\u0026quot;: componentstatus.NewStorage(componentStatusStorage{c.StorageFactory}.serversToValidate), }  InstallLegacyAPIGroup m.GenericAPIServer.InstallLegacyAPIGroup(genericapiserver.DefaultLegacyAPIPrefix, \u0026amp;apiGroupInfo) func (s *GenericAPIServer) InstallLegacyAPIGroup(apiPrefix string, apiGroupInfo *APIGroupInfo) error { if !s.legacyAPIGroupPrefixes.Has(apiPrefix) { return fmt.Errorf(\u0026quot;%q is not in the allowed legacy API prefixes: %v\u0026quot;, apiPrefix, s.legacyAPIGroupPrefixes.List()) } if err := s.installAPIResources(apiPrefix, apiGroupInfo); err != nil { return err } // setup discovery apiVersions := []string{} for _, groupVersion := range apiGroupInfo.GroupMeta.GroupVersions { apiVersions = append(apiVersions, groupVersion.Version) } // Install the version handler. // Add a handler at /\u0026lt;apiPrefix\u0026gt; to enumerate the supported api versions. s.Handler.GoRestfulContainer.Add(discovery.NewLegacyRootAPIHandler(s.discoveryAddresses, s.Serializer, apiPrefix, apiVersions, s.requestContextMapper).WebService()) return nil }  整个方法如上，主要两步，首先调用 installAPIResources(is a private method for installing the REST storage backing each api groupversionresource) 安装 API。\ninstallAPIResources 会遍历apiGroupInfo下的所有 groupVersion, 然后通过s.getAPIGroupVersion得到该 version 下所有的 Storage, 即上面apiGroupInfo.VersionedResourcesStorageMap[groupVersion.Version] map 中所对应的所有 Storage。并通过InstallREST注册到 REST API 的 Handler 中。InstallREST方法如下，在 installer.Install()方法中, 以上面的restStorageMap的 key 为 path, 将所有 Storage 通过registerResourceHandlers(具体方法在 k8s.io\\apiserver\\pkg\\endpoints\\installer.go, 一个近700行的 swich-case 的方法，有兴趣可以看下。)方法注册到 gorestful 的 WebService Route中，并返回一个*metav1.APIResource对象，Install 方法会返回所有 Storage 的生成的 APIResources 和注册到的 WebService。\n接着获取 GroupVersions 中的所有版本并注册到 GoRestfulContainer 中(adds a service to return the supported api versions at the legacy /api), 返回可支持 API 版本。\n// InstallREST registers the REST handlers (storage, watch, proxy and redirect) into a restful Container. // It is expected that the provided path root prefix will serve all operations. Root MUST NOT end // in a slash. func (g *APIGroupVersion) InstallREST(container *restful.Container) error { prefix := path.Join(g.Root, g.GroupVersion.Group, g.GroupVersion.Version) installer := \u0026amp;APIInstaller{ group: g, prefix: prefix, minRequestTimeout: g.MinRequestTimeout, enableAPIResponseCompression: g.EnableAPIResponseCompression, } apiResources, ws, registrationErrors := installer.Install() versionDiscoveryHandler := discovery.NewAPIVersionHandler(g.Serializer, g.GroupVersion, staticLister{apiResources}, g.Context) versionDiscoveryHandler.AddToWebService(ws) container.Add(ws) return utilerrors.NewAggregate(registrationErrors) } // Install handlers for API resources. func (a *APIInstaller) Install() ([]metav1.APIResource, *restful.WebService, []error) { var apiResources []metav1.APIResource var errors []error ws := a.newWebService() proxyHandler := (\u0026amp;handlers.ProxyHandler{ Prefix: a.prefix + \u0026quot;/proxy/\u0026quot;, Storage: a.group.Storage, Serializer: a.group.Serializer, Mapper: a.group.Context, }) // Register the paths in a deterministic (sorted) order to get a deterministic swagger spec. paths := make([]string, len(a.group.Storage)) var i int = 0 for path := range a.group.Storage { paths[i] = path i++ } sort.Strings(paths) for _, path := range paths { apiResource, err := a.registerResourceHandlers(path, a.group.Storage[path], ws, proxyHandler) if err != nil { errors = append(errors, fmt.Errorf(\u0026quot;error in registering resource: %s, %v\u0026quot;, path, err)) } if apiResource != nil { apiResources = append(apiResources, *apiResource) } } return apiResources, ws, errors }  m.InstallAPIs m.InstallAPIs(c.ExtraConfig.APIResourceConfigSource, c.GenericConfig.RESTOptionsGetter, restStorageProviders...) // InstallAPIs will install the APIs for the restStorageProviders if they are enabled. func (m *Master) InstallAPIs(apiResourceConfigSource serverstorage.APIResourceConfigSource, restOptionsGetter generic.RESTOptionsGetter, restStorageProviders ...RESTStorageProvider) {}  用于注册/apis下的 API。在调用 InstallAPIs 之前，会创建/apis下 Storage 的 RESTStorageProvider, 该 interface 的定义及创建在下面代码片段1贴出。\n每个 RESTStorageProvider, 都会有一个NewRESTStorage方法来创建对应资源的 Storage。调用 InstallAPIs 方法时，会将 restStorageProviders 列表传入。 InstallAPIs 会遍历传入的 restStorageProviders 列表，并调用每个 restStorageProvider 的 NewRESTStorage。\nNewRESTStorage方法, 会新建一个 APIGroupInfo, 然后针对 enable 的 API 版本, 调用 VXXStorage 获取对应版本的 ResourcesStorageMap 并存入 apiGroupInfo.VersionedResourcesStorageMap[VXX]中。用来获取 ResourcesStorageMap 的方法，和上面 InstallLegacyAPIGroup.NewLegacyRESTStorage 方法中一样，也是 NewREST, 具体逻辑也基本相同，返回一个 REST 结构体提供对资源的 CRUD 等操作。\nNewRESTStorage方法最终返回的 apiGroupInfo, 会被放入一个 apiGroupsInfo 列表，最后会遍历这个列表并针对每一个 apiGroupInfo 执行 m.GenericAPIServer.InstallAPIGroup(\u0026amp;apiGroupsInfo[i]), 这部分逻辑和 InstallLegacyAPIGroup 一样，通过调用 installAPIResources 将 API 注册到 GoRestfulContainer 中，详细的可以对照上面的 InstallLegacyAPIGroup 的分析参考源码。\n// 代码片段 1 // RESTStorageProvider is a factory type for REST storage. type RESTStorageProvider interface { GroupName() string NewRESTStorage(apiResourceConfigSource serverstorage.APIResourceConfigSource, restOptionsGetter generic.RESTOptionsGetter) (genericapiserver.APIGroupInfo, bool) } // The order here is preserved in discovery. // If resources with identical names exist in more than one of these groups (e.g. \u0026quot;deployments.apps\u0026quot;\u0026quot; and \u0026quot;deployments.extensions\u0026quot;), // the order of this list determines which group an unqualified resource name (e.g. \u0026quot;deployments\u0026quot;) should prefer. // This priority order is used for local discovery, but it ends up aggregated in `k8s.io/kubernetes/cmd/kube-apiserver/app/aggregator.go // with specific priorities. // TODO: describe the priority all the way down in the RESTStorageProviders and plumb it back through the various discovery // handlers that we have. restStorageProviders := []RESTStorageProvider{ authenticationrest.RESTStorageProvider{Authenticator: c.GenericConfig.Authenticator}, authorizationrest.RESTStorageProvider{Authorizer: c.GenericConfig.Authorizer, RuleResolver: c.GenericConfig.RuleResolver}, autoscalingrest.RESTStorageProvider{}, batchrest.RESTStorageProvider{}, certificatesrest.RESTStorageProvider{}, extensionsrest.RESTStorageProvider{}, networkingrest.RESTStorageProvider{}, policyrest.RESTStorageProvider{}, rbacrest.RESTStorageProvider{Authorizer: c.GenericConfig.Authorizer}, schedulingrest.RESTStorageProvider{}, settingsrest.RESTStorageProvider{}, storagerest.RESTStorageProvider{}, // keep apps after extensions so legacy clients resolve the extensions versions of shared resource names. // See https://github.com/kubernetes/kubernetes/issues/42392 appsrest.RESTStorageProvider{}, admissionregistrationrest.RESTStorageProvider{}, eventsrest.RESTStorageProvider{TTL: c.ExtraConfig.EventTTL}, } "
},
{
	"uri": "/cloud/kubelet/",
	"title": "Kubelet",
	"tags": [],
	"description": "",
	"content": "Kubelet 源码阅读笔记\n"
},
{
	"uri": "/cloud/%E9%98%BF%E9%87%8C%E4%BA%91%E4%B8%BB%E6%9C%BA%E6%90%AD%E5%BB%BA-k8s-%E9%9B%86%E7%BE%A4/",
	"title": "阿里云主机搭建 K8S 集群",
	"tags": ["cloud", "k8s"],
	"description": "",
	"content": "通过阿里云ECS实例搭建K8S集群\n环境  阿里云ECS * 2 centos7.4 阿里云两台机器需要内网互通（同一k可用区可以创建免费VPC高速通道实现）  安装 docker 官方文档\nsudo yum install -y yum-utils \\ device-mapper-persistent-data \\ lvm2 sudo yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo sudo yum install docker-ce  配置代理  shadowsocks服务端 provixy shadowsocks客户端 sslocal  安装 shadowsocks yum -y install python-pip pip install shadowsocks  配置 shadowsocks vim /etc/shadowsocks.json  { \u0026quot;server\u0026quot;:\u0026quot;1.1.1.1\u0026quot;, //shadowsocks server ip \u0026quot;server_port\u0026quot;:8888,\t//shadowsocks server port \u0026quot;local_address\u0026quot;: \u0026quot;127.0.0.1\u0026quot;,\t\u0026quot;local_port\u0026quot;:1080,\t//default 1080 \u0026quot;password\u0026quot;:\u0026quot;ssserver_passwd\u0026quot;, \u0026quot;timeout\u0026quot;:300, \u0026quot;method\u0026quot;:\u0026quot;aes-256-cfb\u0026quot;, \u0026quot;fast_open\u0026quot;: false, \u0026quot;workers\u0026quot;: 1 }  安装 privoxy 配置全局代理或 gfwlist 代理\nyum -y install privoxy # 全局代理 echo 'forward-socks5 / 127.0.0.1:1080 .' \u0026gt;\u0026gt;/etc/privoxy/config # gfwlist 代理 # 获取 gfwlist2privoxy 脚本 curl -4sSkL https://raw.github.com/zfl9/gfwlist2privoxy/master/gfwlist2privoxy -O # 生成 gfwlist.action 文件 bash gfwlist2privoxy '127.0.0.1:1080' # 检查 gfwlist.action 文件 more gfwlist.action # 一般有 5000+ 行 # 应用 gfwlist.action 文件 mv -f gfwlist.action /etc/privoxy echo 'actionsfile gfwlist.action' \u0026gt;\u0026gt;/etc/privoxy/config  配置快捷命令 在 /etc/profile.d 新建 set_proxy.sh, linux开机会自动执行该目录下可执行文件\nvim /etc/profile.d/set_proxy.sh  [root@localhost ~]$ cat /etc/profile.d/set_proxy.sh # Initialization script for bash and sh # export proxy for GFW alias proxy_on='nohup sslocal -c /etc/shadowsocks.json \u0026amp; systemctl start privoxy' alias proxy_off='systemctl stop privoxy \u0026amp;\u0026amp; pkill sslocal' alias proxy_export='export http_proxy=http://127.0.0.1:8118 \u0026amp;\u0026amp; export https_proxy=http://127.0.0.1:8118 \u0026amp;\u0026amp; export no_proxy=localhost' alias proxy_unset='unset http_proxy https_proxy no_proxy' alias proxy_test='curl google.com'  手动执行 /etc/profile, 会重新执行/etc/profile.d下文件\nsource /etc/profile  执行alias查看，发现有proxy前缀的别名，则配置成功\n[root@localhost ~]$ alias alias egrep='egrep --color=auto' alias fgrep='fgrep --color=auto' alias grep='grep --color=auto' alias l.='ls -d .* --color=auto' alias ll='ls -l --color=auto' alias ls='ls --color=auto' alias proxy_export='export http_proxy=http://127.0.0.1:8118 \u0026amp;\u0026amp; export https_proxy=http://127.0.0.1:8118 \u0026amp;\u0026amp; export no_proxy=localhost' alias proxy_off='systemctl stop privoxy \u0026amp;\u0026amp; pkill sslocal' alias proxy_on='nohup sslocal -c /etc/shadowsocks.json \u0026amp; systemctl start privoxy' alias proxy_test='curl google.com' alias proxy_unset='unset http_proxy https_proxy no_proxy' alias vi='vim' alias which='alias | /usr/bin/which --tty-only --read-alias --show-dot --show-tilde'  执行下面命令开启代理，并配置环境变量(只对当前shell生效，若要永久生效需要在/etc/proxy中export环境变量)\nproxy_on \u0026amp;\u0026amp; proxy_export  执行 proxy_test 测试代理是否配置成功，出现如下输出则配置成功。\n[root@mqd1c2g ~]$ proxy_test \u0026lt;HTML\u0026gt;\u0026lt;HEAD\u0026gt;\u0026lt;meta http-equiv=\u0026quot;content-type\u0026quot; content=\u0026quot;text/html;charset=utf-8\u0026quot;\u0026gt; \u0026lt;TITLE\u0026gt;301 Moved\u0026lt;/TITLE\u0026gt;\u0026lt;/HEAD\u0026gt;\u0026lt;BODY\u0026gt; \u0026lt;H1\u0026gt;301 Moved\u0026lt;/H1\u0026gt; The document has moved \u0026lt;A HREF=\u0026quot;http://www.google.com/\u0026quot;\u0026gt;here\u0026lt;/A\u0026gt;. \u0026lt;/BODY\u0026gt;\u0026lt;/HTML\u0026gt;  参考: ss-local 终端代理（gfwlist）\n安装 Kubernetes 官方文档\n安装kubeadm 检查机器是否符合文档中的Before you begin的要求, 符合的话才能进行接下来的步骤。\ncat \u0026lt;\u0026lt;EOF \u0026gt; /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg exclude=kube* EOF # Set SELinux in permissive mode (effectively disabling it) setenforce 0 sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes systemctl enable kubelet \u0026amp;\u0026amp; systemctl start kubelet  创建 master 节点 # pod-network-cidr 10.244.0.0/16 是后面 flannel 默认配置的 pod Network，配置成这个地址不用改的flannel的 默认配置 kubeadm init --pod-network-cidr 10.244.0.0/16  成功执行后 master 节点就已经启动了, 可以选择安装一种网络插件，这里选择flannel\n# 使用默认配置启动 flannel 的 DaemonSet kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml  创建 node 节点 在 master 节点执行下面命令生成创建子节点命令\nkubeadm token create --print-join-command  [root@localhost ~]$ kubeadm token create --print-join-command kubeadm join masterip:6443 --token 5zk5ql.5eq0rgoui0dl0xx3 --discovery-token-ca-cert-hash sha256:5bef3894fc492bf9d93c9f248f84ec3sdsadasdss7685191a9d841fd32a88bb9ac9  在 node 节点执行上述命令生成的命令\nkubeadm join masterip:6443 --token 5zk5ql.5eq0rgoui0dl0xx3 --discovery-token-ca-cert-hash sha256:5bef3894fc492bf9d93c9f248f84ec3sdsadasdss7685191a9d841fd32a88bb9ac9  执行成功则 node 节点添加成功\n"
},
{
	"uri": "/contact/",
	"title": "Contact",
	"tags": [],
	"description": "",
	"content": " email: maoqidemail@gmail.com "
},
{
	"uri": "/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Maoqide learning.\nrecord.\ncollect.\n"
},
{
	"uri": "/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/cloud/",
	"title": "Cloud",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/k8s/",
	"title": "K8s",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/notes/",
	"title": "Notes",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]